1
00:00:06,656 --> 00:00:08,192
Welcome collector 15

2
00:00:08,704 --> 00:00:13,568
We're going to start talking about a completely new topic now namely neural networks

3
00:00:14,336 --> 00:00:15,104
And

4
00:00:15,616 --> 00:00:16,128
I want to

5
00:00:16,384 --> 00:00:17,920
Kick-off by

6
00:00:18,176 --> 00:00:19,712
First mentioning where in the book

7
00:00:19,968 --> 00:00:25,856
We are going to be looking at Chapters 3839 on 42 and the sum of that chapter has mentioned that

8
00:00:26,112 --> 00:00:27,904
But you could take a look at 2

9
00:00:29,440 --> 00:00:29,952
And

10
00:00:30,464 --> 00:00:32,768
I want to motivate why we should be

11
00:00:33,280 --> 00:00:35,584
Interested in neural networks O'Toole

12
00:00:35,840 --> 00:00:39,424
I know. I talked about brains which is one of the

13
00:00:39,680 --> 00:00:44,800
Motivations are we don't understand how brains book and so it's interesting to look at theoretical models

14
00:00:45,312 --> 00:00:46,592
Open

15
00:00:47,360 --> 00:00:50,432
Neural networks do have engineering applications as well

16
00:00:50,688 --> 00:00:53,248
So even if you don't care about your science you might be interested

17
00:00:53,504 --> 00:00:54,528
Infolding

18
00:00:54,784 --> 00:00:56,064
Machine learning problems

19
00:00:56,320 --> 00:00:57,856
Using your last purchase

20
00:00:58,112 --> 00:01:00,672
But let's talk about brands

21
00:01:01,184 --> 00:01:02,208
The first

22
00:01:02,464 --> 00:01:08,352
Amazing thing about brains which we will come to in the second lecture on neural networks

23
00:01:09,120 --> 00:01:13,728
Is our amazing ability to do content addressable memory and this is

24
00:01:13,984 --> 00:01:15,520
Quite a remarkable thing that we

25
00:01:15,776 --> 00:01:17,568
Take for granted because we do it all the time

26
00:01:18,080 --> 00:01:22,944
Let me just remind you of what I mean by content addressable memory

27
00:01:23,200 --> 00:01:24,224
So I can give you

28
00:01:24,736 --> 00:01:30,624
Images that are noisy and blurry and I have a small number of pixels and maybe they make you think of things

29
00:01:30,880 --> 00:01:36,000
And we get at the content we get at the memory that you recall when you see the image

30
00:01:36,256 --> 00:01:37,280
By me showing you

31
00:01:37,792 --> 00:01:39,328
Fragment an incomplete

32
00:01:39,584 --> 00:01:41,632
Partial content of that memory

33
00:01:42,144 --> 00:01:48,288
And that's so different from the waist down the computers what cast on the computer you know the memory is in a drawer in the file

34
00:01:48,544 --> 00:01:53,664
Get the memory you need to know which filing cabinet number on which card it's on and then you go and find it there

35
00:01:54,176 --> 00:02:00,064
He was saying what was the cartoon that has these holes in it so magically outcomes the card

36
00:02:01,600 --> 00:02:07,488
Are you with me can you see what amount of Atlas give you justice Inglewood to

37
00:02:07,744 --> 00:02:10,304
And another black and white and it's not full color

38
00:02:10,560 --> 00:02:16,704
Something with numbers 0 4-9 don't know why but

39
00:02:16,960 --> 00:02:18,752
That's just

40
00:02:19,008 --> 00:02:21,312
37 squared

41
00:02:21,568 --> 00:02:27,712
I'll give you a little q's and you actually come up with the entire memory from the cues

42
00:02:27,968 --> 00:02:30,272
An amazing thing the Brain Stew

43
00:02:30,528 --> 00:02:32,064
The me it's really

44
00:02:32,832 --> 00:02:38,976
The most exciting thing about neural networks is that Brayden can do this and we have some models that have a little bit of

45
00:02:39,232 --> 00:02:41,536
Bissell capability which will come to in the next election

46
00:02:43,072 --> 00:02:49,216
Let me give another example I will tell you about an oscar-nominated actress but I wouldn't reveal her whole name

47
00:02:49,472 --> 00:02:53,312
She acted as amazingly good to George Lucas film

48
00:02:53,824 --> 00:02:58,944
Even though one of the pieces of information that I just giving you is incorrect

49
00:02:59,200 --> 00:03:02,016
Nevertheless you all thinking of Natalie Portman my prediction

50
00:03:03,040 --> 00:03:06,624
So here's another example I'm going to show you the cover of a book

51
00:03:06,880 --> 00:03:13,024
And the title has several words in it and I'm going to blank at all of the title except for two letters in the title

52
00:03:13,280 --> 00:03:16,096
Going to blank out almost full of the the cover

53
00:03:16,352 --> 00:03:21,216
And I'm going to put that black patch over the eyes of the author baby offers on the cover

54
00:03:21,472 --> 00:03:25,056
Because when you put a black patch over someone's eyes then they're very hard to recognize

55
00:03:25,568 --> 00:03:26,592
But nevertheless

56
00:03:26,848 --> 00:03:28,384
What was I can look

57
00:03:30,176 --> 00:03:31,968
A Brief History of Time well done

58
00:03:32,224 --> 00:03:35,040
Saturday

59
00:03:36,064 --> 00:03:41,440
How do we do it can we get machines to do it how do brains do it for me. So really

60
00:03:41,696 --> 00:03:43,744
Compelling interesting and exciting problem

61
00:03:44,512 --> 00:03:50,656
We might discuss that today but it's one of the motivations for looking at neural networks so today we'll do some basic stuff with neural

62
00:03:50,912 --> 00:03:51,680
Networks and maybe

63
00:03:51,936 --> 00:03:53,728
Look at some engineering applications open

64
00:03:54,240 --> 00:03:59,104
And this is going to be the Pinnacle of the neural Nets bet we'll come back to this question

65
00:03:59,360 --> 00:04:05,504
How can we do content to this address for memory and I'll show you a solution using neural networks

66
00:04:05,760 --> 00:04:08,064
Let me give you just one more

67
00:04:08,320 --> 00:04:12,672
Motivation for being excited about brains and neural Nets in general

68
00:04:13,184 --> 00:04:19,327
This is based on some experiments that work as follows the experiments are conducted in 1989

69
00:04:19,839 --> 00:04:25,727
The subjects in the experiment for showing 725 images and four of example images

70
00:04:25,983 --> 00:04:27,775
Showing on the screen now

71
00:04:28,543 --> 00:04:31,615
100 of them were labeled as positive images

72
00:04:31,871 --> 00:04:33,663
And 625 of them

73
00:04:33,919 --> 00:04:37,247
Well labeled negative images so the subject was

74
00:04:37,503 --> 00:04:39,295
Instructed to memorize

75
00:04:39,551 --> 00:04:41,343
The label plus plus or minus

76
00:04:41,599 --> 00:04:44,927
Early images were fairly similar in character to each other

77
00:04:45,695 --> 00:04:51,839
Several months later the subjects could remember all of the positive and negative examples and could distinguish them

78
00:04:52,351 --> 00:04:58,495
From other images that had not been seen earlier in the experiment and even 12 months later they could still correctly

79
00:04:58,751 --> 00:05:01,567
Recall all the positive and negative labels

80
00:05:03,103 --> 00:05:03,615
Band

81
00:05:03,871 --> 00:05:06,431
This is impressive to me because the subjects were pigeons

82
00:05:07,199 --> 00:05:07,967
And

83
00:05:08,991 --> 00:05:11,551
Pigeons can learn random binary labels

84
00:05:11,807 --> 00:05:17,951
Completely arbitre buy new labels associated with images they can also learn to identify a particular person

85
00:05:18,207 --> 00:05:23,327
The Ashley the four images that the two on the right both have the same goal with the long hair and and Boots

86
00:05:23,583 --> 00:05:24,863
And

87
00:05:25,119 --> 00:05:29,983
So that's how some of the images working some of these experiments

88
00:05:30,495 --> 00:05:33,567
Whether it's running labels on meaningful labels pigeons can learn them

89
00:05:33,823 --> 00:05:39,455
And they can respond within a fraction of a second and correctly cite you have I seen this image before

90
00:05:40,223 --> 00:05:42,783
And that ability to recognize something familiar

91
00:05:43,039 --> 00:05:45,599
It's something that we still struggle to do

92
00:05:45,855 --> 00:05:51,999
With modern computers so I'm going to spend a little bit more motivational times saying what is the difference

93
00:05:52,255 --> 00:05:54,047
What's between a pigeon and a supercomputer

94
00:05:59,935 --> 00:06:00,703
So

95
00:06:01,727 --> 00:06:03,263
First let's just talk about

96
00:06:03,775 --> 00:06:05,567
The hardware differences

97
00:06:05,823 --> 00:06:11,711
Magma basic case there is a pigeon's fresh supercomputers pigeons are much better than craze

98
00:06:11,967 --> 00:06:16,319
At this sort of task of recognizing and responding to images extremely quickly

99
00:06:18,879 --> 00:06:22,975
So is it because pigeons actually have access to more Hardware

100
00:06:25,535 --> 00:06:27,071
Let's count

101
00:06:29,887 --> 00:06:34,751
The number of devices in a pigeon brain is about 10 ft 11 neurons

102
00:06:36,031 --> 00:06:42,175
The number of cynopsis the neuron might be 10 to the three also so maybe the 10 to the 14th cynopsis

103
00:06:42,431 --> 00:06:43,967
I need to think of either the neurons

104
00:06:44,223 --> 00:06:48,319
All the sign ups is being Elementary device is a little bit like transistors

105
00:06:49,343 --> 00:06:53,183
The number of devices in the cray

106
00:06:53,695 --> 00:06:54,719
I don't know

107
00:06:56,255 --> 00:06:58,047
Siri say 10 to the 10

108
00:06:58,815 --> 00:07:01,631
Bits of memory

109
00:07:04,191 --> 00:07:07,775
That's if you got one gig of memory of course I have palm oil

110
00:07:08,543 --> 00:07:14,687
But I don't want to be unnecessarily on sad because my resources are not being completely on used what about in the sea

111
00:07:14,943 --> 00:07:15,711
CPU itself

112
00:07:16,223 --> 00:07:21,343
The CPU maybe is I don't know a million also devices in a single

113
00:07:22,623 --> 00:07:23,391
CPU

114
00:07:24,927 --> 00:07:29,279
How so you could say hi ya the pigeon is Beats the clay because it's got more devices

115
00:07:29,791 --> 00:07:35,679
But the pigeons devices are slow so we better take into account the clock rate

116
00:07:37,983 --> 00:07:38,751
Find

117
00:07:39,263 --> 00:07:41,055
5 for a crate

118
00:07:41,311 --> 00:07:42,079
I'd say

119
00:07:43,103 --> 00:07:45,663
The clock right is something like a thousand megahertz

120
00:07:46,175 --> 00:07:47,455
Which means that

121
00:07:47,967 --> 00:07:50,015
Where in the ballpark of

122
00:07:50,527 --> 00:07:54,111
Having 10 + 15/10 + 19

123
00:07:55,135 --> 00:07:56,159
Device

124
00:07:56,927 --> 00:07:58,207
Operations

125
00:08:00,511 --> 00:08:01,535
II

126
00:08:02,303 --> 00:08:06,143
I just checked the latest news on two computers is in

127
00:08:06,399 --> 00:08:09,727
Super videos with lots of CPUs and tons of memory

128
00:08:10,239 --> 00:08:11,263
And they can do

129
00:08:11,519 --> 00:08:13,311
10 to the 16

130
00:08:13,567 --> 00:08:16,383
Floating Point operations

131
00:08:17,151 --> 00:08:18,431
II

132
00:08:19,711 --> 00:08:25,855
So we're just two different ways of of quantifying how much you've got in a cray so this is

133
00:08:26,111 --> 00:08:28,159
In terms of the the hardware

134
00:08:30,975 --> 00:08:37,119
How many Hardware device operations you got per second and then it says in terms of the outfit what entire

135
00:08:38,399 --> 00:08:40,191
Supercomputer

136
00:08:40,447 --> 00:08:41,215
A single

137
00:08:41,727 --> 00:08:43,007
Not cry anymore

138
00:08:43,775 --> 00:08:44,287
This is

139
00:08:45,055 --> 00:08:46,591
Today's pop soup

140
00:08:46,847 --> 00:08:48,127
Supercomputer

141
00:08:50,943 --> 00:08:54,015
Crank up 1016 floating Point operations per second

142
00:08:54,783 --> 00:08:55,807
What about the pigeon

143
00:08:56,063 --> 00:09:00,159
Well clock rate correspondence to attend milliseconds

144
00:09:00,671 --> 00:09:06,815
Clock time I'd say roughly cuz a single neuron can fire a hundred times a second if you push it

145
00:09:09,375 --> 00:09:14,495
You could argue that some bits of the hardware and maybe have a faster clock time maybe

146
00:09:14,751 --> 00:09:19,871
I'm an advocate of the idea to do some very interesting things but

147
00:09:20,127 --> 00:09:21,919
Let's run with this number anyway

148
00:09:22,431 --> 00:09:25,503
That means you got 10 to the 13 operations

149
00:09:27,039 --> 00:09:29,343
II

150
00:09:29,599 --> 00:09:32,415
If you take the neurons as your

151
00:09:32,671 --> 00:09:33,951
Operating devices

152
00:09:35,743 --> 00:09:37,279
10 to the 16

153
00:09:38,815 --> 00:09:40,095
Operations per second

154
00:09:42,911 --> 00:09:45,471
If you view the elementary transistor like

155
00:09:45,983 --> 00:09:46,751
Objects

156
00:09:47,519 --> 00:09:49,823
I was being sorry for the cynopsis

157
00:09:53,151 --> 00:09:55,711
Sentence of operations per second

158
00:09:56,223 --> 00:10:00,575
Device operations per second the cray has got more going for it

159
00:10:01,087 --> 00:10:02,367
Time the pigeon

160
00:10:02,623 --> 00:10:05,439
It doesn't matter output floating operations per second

161
00:10:05,695 --> 00:10:08,767
If you take the sign-ups is doing floating-point operations

162
00:10:10,303 --> 00:10:16,447
There are the same level as a a supercomputer in a single pigeon brain doesn't

163
00:10:16,703 --> 00:10:19,519
An amazing computational ability in a pigeon brain

164
00:10:19,775 --> 00:10:23,103
But it's not actually bigger in terms of these numbers

165
00:10:23,359 --> 00:10:24,383
Then I cry yes

166
00:10:26,943 --> 00:10:32,319
Okay the question is what's the size of the words so you're saying for these floating Point operations

167
00:10:32,831 --> 00:10:33,599
Haha

168
00:10:33,855 --> 00:10:39,999
He is a floating-point operation in a cry so probably that sums of operation long lines over 30

169
00:10:40,255 --> 00:10:44,095
32 bit integer getting X another 32

170
00:10:44,351 --> 00:10:45,887
Bit integer or something like that

171
00:10:46,143 --> 00:10:48,703
Is an absolutely perfect

172
00:10:49,215 --> 00:10:50,495
Plaza sign ups

173
00:10:50,751 --> 00:10:53,055
If you're lucky is probably doing

174
00:10:53,567 --> 00:10:55,871
I may be too old full bits

175
00:10:58,431 --> 00:10:59,455
The operation

176
00:10:59,967 --> 00:11:06,111
I'll guess I'll be surprised if I'm on a time schedule just 10 minutes seconds maybe getting into a full bits

177
00:11:06,367 --> 00:11:10,719
Weather channels are open or not

178
00:11:10,975 --> 00:11:12,511
Integer numbers of channels

179
00:11:13,279 --> 00:11:13,791
Okay

180
00:11:14,559 --> 00:11:18,911
So yes the wood size is only the Precision of these operations

181
00:11:19,167 --> 00:11:20,447
Is bigger for the Christ

182
00:11:21,215 --> 00:11:23,263
Alpha today supercomputer

183
00:11:23,775 --> 00:11:26,591
Then facilitation

184
00:11:27,871 --> 00:11:31,455
And it's not the case of the pigeon has far more results is at its disposal

185
00:11:34,015 --> 00:11:39,903
Maybe the pigeon is far better at this image recognizing task because it organized its Hardware differently

186
00:11:40,671 --> 00:11:41,183
So

187
00:11:47,583 --> 00:11:48,863
Supercomputers

188
00:11:49,119 --> 00:11:53,215
Are essentially just standard serial computers wired up in a slightly

189
00:11:53,727 --> 00:11:56,799
Different weight they have a CPU

190
00:11:57,311 --> 00:11:58,847
Or lots of CPUs

191
00:12:01,407 --> 00:12:03,455
If you want to store a new memory

192
00:12:04,479 --> 00:12:10,623
Without losing any of your old memories in a standard computer you have to have some new virgin

193
00:12:10,879 --> 00:12:11,903
In Hardware

194
00:12:17,791 --> 00:12:23,935
So does no connection at all between old memories and you married to just put them in another place and you need to know

195
00:12:24,191 --> 00:12:24,703
Weather

196
00:12:27,007 --> 00:12:29,567
Meanwhile pigeon brains have

197
00:12:31,871 --> 00:12:33,151
The parallel

198
00:12:33,407 --> 00:12:35,455
I have high conductivity

199
00:12:36,479 --> 00:12:38,783
So

200
00:12:39,295 --> 00:12:44,415
A typical neuron is probably connected to about a thousand of a neurons

201
00:12:44,671 --> 00:12:46,975
Where are the typical transistor

202
00:12:47,231 --> 00:12:49,791
And I cry it's probably connected

203
00:12:50,303 --> 00:12:52,863
2

204
00:12:53,119 --> 00:12:55,423
Roughly 3 or 10

205
00:12:55,679 --> 00:12:56,703
All the transistors

206
00:12:57,471 --> 00:12:57,983
Typically

207
00:13:01,311 --> 00:13:04,895
The computation however is being done

208
00:13:05,151 --> 00:13:08,991
Not the way I understand it is definitely distributed

209
00:13:10,783 --> 00:13:12,319
When a

210
00:13:12,831 --> 00:13:18,975
Pigeon learn some new memory then you memories go in the same Hardware that's using

211
00:13:19,231 --> 00:13:21,023
For everything else

212
00:13:22,047 --> 00:13:22,815
We don't know how

213
00:13:23,839 --> 00:13:25,375
But that's told in the same Hardware

214
00:13:28,191 --> 00:13:29,727
And importantly

215
00:13:34,591 --> 00:13:37,151
Pigeon brains are Robusta Hardware

216
00:13:37,407 --> 00:13:39,967
Damage so you can take a pigeon brain

217
00:13:40,223 --> 00:13:45,343
We can take your brain and you can damage many thousands of your neurons and you still

218
00:13:45,599 --> 00:13:46,367
The following day

219
00:13:46,623 --> 00:13:50,207
Current functioning okay you do this whenever you drink alcohol

220
00:13:50,719 --> 00:13:54,047
You kill us in your arms but you still keep going

221
00:13:54,559 --> 00:14:00,703
The Kray in contrast if you reach into it inside while going to destroy 1% of the transit

222
00:14:00,959 --> 00:14:05,567
Does he miss cry it's going to be fine isn't it you find that it is not fine

223
00:14:06,591 --> 00:14:09,407
So here's the hypothesis

224
00:14:09,663 --> 00:14:11,199
The hypothesis is

225
00:14:12,479 --> 00:14:13,759
Maybe

226
00:14:15,807 --> 00:14:20,671
The difference in the performance of the pigeon and the crayon this realistic

227
00:14:20,927 --> 00:14:23,487
Real world image recognition task

228
00:14:23,999 --> 00:14:24,511
Is

229
00:14:25,535 --> 00:14:28,351
Because of the difference in style of computation

230
00:14:30,911 --> 00:14:32,959
But maybe the style of computation

231
00:14:35,775 --> 00:14:37,567
Is the key

232
00:14:39,871 --> 00:14:46,015
So maybe we should be getting away from cereal if we're excited about being able to solve problems that computers are still useless

233
00:14:46,271 --> 00:14:49,855
I maybe we should be going to genuinely parallel

234
00:14:51,647 --> 00:14:55,743
Maybe we should be looking at ways of using Hardware that you have high conductivity

235
00:14:55,999 --> 00:14:56,767
Doll Distributing

236
00:14:57,279 --> 00:14:58,815
And work in a completely different way

237
00:15:01,887 --> 00:15:05,471
Sorry the next lecture we will come back to the the task of

238
00:15:05,727 --> 00:15:07,263
Storing and recalling memories

239
00:15:07,519 --> 00:15:09,567
Anvil show a way of doing it

240
00:15:11,103 --> 00:15:12,639
With a simple neural network models

241
00:15:14,431 --> 00:15:14,943
So

242
00:15:15,455 --> 00:15:16,991
I've just giving you a

243
00:15:17,247 --> 00:15:23,391
15 minute motivation for why we should be interested in parallel distributed

244
00:15:23,647 --> 00:15:27,743
Which is the name of one of the old Bibles of neural networks

245
00:15:37,727 --> 00:15:41,567
And this one parallel distributed processing I'm going to talk about is

246
00:15:42,591 --> 00:15:44,895
Parallel processing using

247
00:15:46,175 --> 00:15:48,991
Elementary devices that will cool neurons

248
00:15:49,503 --> 00:15:51,551
So don't have a single neuron

249
00:15:54,111 --> 00:15:56,159
We just going to be a thing to have some input

250
00:15:57,439 --> 00:16:00,255
Unlock put

251
00:16:01,023 --> 00:16:03,583
And then we'll go to wire them up in a variety of ways

252
00:16:09,471 --> 00:16:13,055
1 way wiring them up is feed-forward

253
00:16:16,895 --> 00:16:20,223
Which looks like this you have some inputs going in

254
00:16:23,039 --> 00:16:24,063
Sonny Rollins

255
00:16:24,319 --> 00:16:29,951
And then they can access to some money runs and we can put arrows on these I just to show which way things go

256
00:16:33,023 --> 00:16:36,095
And then maybe have another neuron and then something comes out

257
00:16:36,607 --> 00:16:38,911
So that's a feed-forward

258
00:16:39,167 --> 00:16:39,935
Network

259
00:16:40,447 --> 00:16:46,591
It's simple to describe because it's so you just put in and put these guys

260
00:16:46,847 --> 00:16:49,151
Computer Discount Computer and then you're done

261
00:16:50,687 --> 00:16:55,551
Another way of watering these things up would be a feedback Network why you say

262
00:16:56,063 --> 00:16:57,599
Let's allowed

263
00:16:57,855 --> 00:16:59,647
Everyone's outputs

264
00:16:59,903 --> 00:17:01,183
To be

265
00:17:01,439 --> 00:17:03,487
Everyone else is inputs

266
00:17:09,119 --> 00:17:11,167
Like that

267
00:17:13,471 --> 00:17:14,239
And then

268
00:17:14,495 --> 00:17:17,055
The Dynamics depend on exactly how you define

269
00:17:17,311 --> 00:17:17,823
The way

270
00:17:18,079 --> 00:17:20,127
Holding your own sand tracks

271
00:17:20,383 --> 00:17:22,431
And maybe something more exciting happens

272
00:17:23,967 --> 00:17:24,479
Suck

273
00:17:25,759 --> 00:17:28,063
Let's talk a little bit more about the single neuron

274
00:17:28,319 --> 00:17:29,087
And then

275
00:17:29,343 --> 00:17:31,647
But today's lecture I'll talk to you about

276
00:17:33,183 --> 00:17:34,463
Seafood Networks

277
00:17:36,255 --> 00:17:38,815
And the next election will look at feedback

278
00:17:39,071 --> 00:17:40,095
Netflix

279
00:18:02,111 --> 00:18:03,391
So his house a

280
00:18:04,159 --> 00:18:05,695
Single neuron wax

281
00:18:07,231 --> 00:18:11,583
Singing the wrong input and an output

282
00:18:15,167 --> 00:18:17,215
And it's got some parameters

283
00:18:17,471 --> 00:18:23,359
And the promises I'll come in the cold weight

284
00:18:26,175 --> 00:18:29,759
And the promise is look here between the input and the

285
00:18:30,015 --> 00:18:32,063
Body of the neuron

286
00:18:32,319 --> 00:18:34,367
And his eye works it's very simple

287
00:18:34,879 --> 00:18:40,255
If the weights W1 W2 trouble. W

288
00:18:40,767 --> 00:18:44,095
Okay and if the inputs on X1

289
00:18:44,607 --> 00:18:46,143
X2

290
00:18:46,399 --> 00:18:47,167
Okay

291
00:18:48,191 --> 00:18:53,823
What the neuron does is it adds up the weighted sum of its inputs

292
00:18:54,335 --> 00:18:55,615
Using its own weight

293
00:18:58,175 --> 00:19:01,759
And add one more number Cole the w0

294
00:19:02,527 --> 00:19:05,087
But you can think all those little dangling extra

295
00:19:06,879 --> 00:19:11,487
White hair connected to a virtual input to this voice.

296
00:19:11,743 --> 00:19:16,095
And this wait here is also sometimes called the bias

297
00:19:16,607 --> 00:19:18,143
Avenue around which is what it

298
00:19:18,655 --> 00:19:23,263
Activation a will equal if all of the inputs

299
00:19:24,031 --> 00:19:24,543
Are there

300
00:19:25,823 --> 00:19:28,383
Then having computed it activation

301
00:19:29,151 --> 00:19:30,687
It computer output

302
00:19:31,711 --> 00:19:37,855
Although sometimes called activity of the neuron

303
00:19:38,111 --> 00:19:38,879
Through a function

304
00:19:43,743 --> 00:19:44,767
And not function

305
00:19:46,559 --> 00:19:47,839
Could be

306
00:19:48,095 --> 00:19:50,399
101 + 89

307
00:19:50,911 --> 00:19:52,447
Which looks like.

308
00:19:55,263 --> 00:19:58,591
Or it could be

309
00:19:59,359 --> 00:20:02,175
Hyperbolic tangent to me

310
00:20:04,223 --> 00:20:06,271
Which looks almost exactly the same

311
00:20:08,831 --> 00:20:11,903
Or it could be a step function

312
00:20:12,415 --> 00:20:13,183
Like that

313
00:20:27,519 --> 00:20:33,407
Something we could note in passing is that we have come across neurons already without

314
00:20:33,663 --> 00:20:34,175
How

315
00:20:34,431 --> 00:20:35,455
Knowing it

316
00:20:37,247 --> 00:20:38,271
They came up

317
00:20:39,295 --> 00:20:44,159
In previous like that when we discuss the variational method for spin systems that we had

318
00:20:44,671 --> 00:20:50,303
Quantity is called a which will the weighted sum of average activities of spins

319
00:20:50,559 --> 00:20:51,839
Rex walls

320
00:20:52,095 --> 00:20:53,887
How much I spend with pointy up

321
00:20:55,167 --> 00:20:59,775
He was activation and then we slept that too attached to determine how much we were pointing up

322
00:21:00,031 --> 00:21:02,335
Stop we have actually seen this

323
00:21:04,639 --> 00:21:07,455
And divide rational free energy

324
00:21:07,711 --> 00:21:09,759
Minimisation

325
00:21:11,039 --> 00:21:14,111
For a spin system

326
00:21:15,903 --> 00:21:18,464
We both have same this

327
00:21:19,488 --> 00:21:21,024
A little before

328
00:21:21,792 --> 00:21:23,584
One of the questions

329
00:21:25,120 --> 00:21:25,888
But I

330
00:21:26,144 --> 00:21:28,960
Maybest in lecture and it's only in the book is

331
00:21:29,216 --> 00:21:31,520
If there are two gaussian distributions

332
00:21:33,824 --> 00:21:35,616
And you don't know which

333
00:21:36,128 --> 00:21:38,176
All those a datapoint comes from

334
00:21:38,944 --> 00:21:41,504
And it comes along and X1

335
00:21:42,016 --> 00:21:43,808
X2 space

336
00:21:46,368 --> 00:21:51,232
And let's say that this guy soon is labeled last one in this one's label kustoo

337
00:21:52,256 --> 00:21:58,400
If I give you a datapoint x that came from this picture to guy shins please tell me what to do thing

338
00:21:58,656 --> 00:22:00,704
Think is the probability that equals 1

339
00:22:01,216 --> 00:22:03,520
The answer to that question is

340
00:22:03,776 --> 00:22:05,056
Apple pay

341
00:22:05,312 --> 00:22:09,664
Where is this thing

342
00:22:09,920 --> 00:22:13,504
And a is indeed of precisely this warm summer

343
00:22:14,016 --> 00:22:20,160
Pub w k x k + w

344
00:22:20,416 --> 00:22:21,184
But the question

345
00:22:24,512 --> 00:22:29,120
What's the inference of what class I mean and the answer is what was 50% chance

346
00:22:29,376 --> 00:22:31,168
On this line

347
00:22:31,424 --> 00:22:32,192
Honda Ohio

348
00:22:32,448 --> 00:22:34,752
Chance on this line in the lotions

349
00:22:35,008 --> 00:22:36,288
Hummus line I'm so full

350
00:22:36,800 --> 00:22:37,312
So

351
00:22:37,568 --> 00:22:40,128
The answer to the question varies

352
00:22:40,640 --> 00:22:42,432
With consoles that are linear

353
00:22:42,688 --> 00:22:43,968
And X1 and X2

354
00:22:44,224 --> 00:22:47,552
Assuming that these two gaussians have identical covariance

355
00:22:47,808 --> 00:22:52,160
Matrices I think we did that exercise when you are talking about clustering earlier

356
00:22:53,952 --> 00:23:00,096
We've already encountered neurons we just didn't call them the time we called them simple mathematical functions

357
00:23:00,352 --> 00:23:00,864
Just what they are

358
00:23:03,168 --> 00:23:08,800
So let's just familiar familiarize ourselves with the new language that were using half

359
00:23:10,336 --> 00:23:13,664
And let's have a play with one neuron

360
00:23:17,248 --> 00:23:19,040
Here is

361
00:23:20,320 --> 00:23:22,368
A picture of the output of one neuron

362
00:23:22,880 --> 00:23:26,208
And the title at the top of it we can turn the lights down

363
00:23:26,976 --> 00:23:27,744
The title

364
00:23:28,000 --> 00:23:28,768
Told you

365
00:23:29,024 --> 00:23:33,120
What the three weights of this neuron ay3

366
00:23:33,376 --> 00:23:34,144
Well

367
00:23:34,400 --> 00:23:36,448
It's because this particular neuron

368
00:23:36,960 --> 00:23:39,264
That was playing with her husband

369
00:23:40,032 --> 00:23:40,800
Inputs

370
00:23:42,848 --> 00:23:43,872
We're going to play

371
00:23:44,128 --> 00:23:45,408
For a while with

372
00:23:45,920 --> 00:23:48,992
The feed Food Network which consists of just one euro

373
00:23:49,760 --> 00:23:50,784
With two inputs

374
00:23:51,296 --> 00:23:52,320
One output

375
00:23:52,832 --> 00:23:56,928
And there is as usual a little bias hanging off here what you can think of

376
00:23:57,696 --> 00:24:00,000
As being a weight w 0

377
00:24:00,256 --> 00:24:02,304
Connected to an input with a white wall

378
00:24:02,816 --> 00:24:05,120
But everyone looks at WW2 luxhair

379
00:24:05,888 --> 00:24:06,656
Right

380
00:24:07,424 --> 00:24:08,960
As a function of x 1 x 2

381
00:24:09,472 --> 00:24:11,008
This is what the output of a neuron

382
00:24:11,264 --> 00:24:12,032
Looks like

383
00:24:12,800 --> 00:24:13,312
If

384
00:24:13,824 --> 00:24:19,712
The weights offsets 2-15 2 and 1 in the old bios white woman Lake to

385
00:24:20,480 --> 00:24:21,248
And

386
00:24:21,760 --> 00:24:27,904
The function that I'm using is the one of the OnePlus 3 to the minus a which is sometimes called the logistic function

387
00:24:33,024 --> 00:24:33,536
Okay

388
00:24:33,792 --> 00:24:37,632
Not at play with late so let's change the weights

389
00:24:37,888 --> 00:24:44,032
For the bias change the bias of the neuron the function just trundles to and fro

390
00:24:44,544 --> 00:24:47,104
The orientation of the cantos doesn't change

391
00:24:47,360 --> 00:24:48,384
But I'm

392
00:24:51,200 --> 00:24:54,784
Function slides around now that change weight to

393
00:24:55,040 --> 00:24:56,064
What does that do

394
00:24:56,576 --> 00:24:59,392
Well it doesn't change with the function

395
00:24:59,648 --> 00:25:00,416
Intersex

396
00:25:00,672 --> 00:25:02,208
The X to equal 0

397
00:25:02,464 --> 00:25:03,232
Access

398
00:25:03,488 --> 00:25:04,000
But it did

399
00:25:04,256 --> 00:25:07,328
Make the function swivel around that point

400
00:25:07,840 --> 00:25:09,632
Dollar exchange rate Wong

401
00:25:10,144 --> 00:25:15,776
That's going to make it swivel around the place where the red curve intersects the X one equals zero

402
00:25:16,032 --> 00:25:16,800
Access

403
00:25:17,056 --> 00:25:20,384
So now you can maybe think of it at full swiveling

404
00:25:20,896 --> 00:25:22,688
Around a point that's off the screen

405
00:25:24,224 --> 00:25:29,856
This is the surface plot review let's redo what I just did Rapunzel plots his Rapunzel plot

406
00:25:30,112 --> 00:25:30,880
4

407
00:25:34,976 --> 00:25:36,768
Sneakers look right size

408
00:25:45,984 --> 00:25:50,848
I'm showing a contour plot of exactly the same function that we had America with the wife set to

409
00:25:51,104 --> 00:25:52,896
-50 + 2 + 1

410
00:25:53,664 --> 00:25:55,712
Here is 0.5 console

411
00:25:55,968 --> 00:26:00,064
Here are the consoles that I forgot .2 + .80 something like that

412
00:26:00,320 --> 00:26:05,184
And the white line is a representation of the normal

413
00:26:05,440 --> 00:26:05,952
2

414
00:26:06,208 --> 00:26:06,720
The red

415
00:26:07,232 --> 00:26:07,744
Cancel

416
00:26:09,792 --> 00:26:13,632
And if you like it's pointing in the direction of the weight Vector so

417
00:26:15,168 --> 00:26:18,496
This is the X1 X2 still is is the X2 Axis

418
00:26:18,752 --> 00:26:20,288
And I'm showing avexa

419
00:26:21,056 --> 00:26:23,616
That's proportional to 2 in this direction

420
00:26:23,872 --> 00:26:26,944
I'm warm in this direction so proportional to the weight vector

421
00:26:27,712 --> 00:26:28,480
All right

422
00:26:28,736 --> 00:26:33,856
Strictly these vectors input space X1 X2 and weights

423
00:26:34,112 --> 00:26:39,744
31W Jewel spaces to each other so we shouldn't necessarily go forcing them on the same

424
00:26:40,000 --> 00:26:43,584
Sunscreen that's why I stretch the access to make make it look

425
00:26:43,840 --> 00:26:46,656
Perpendicular

426
00:26:47,424 --> 00:26:48,704
Okay so let's go through the

427
00:26:48,960 --> 00:26:55,104
Three-way twiddling things we just did I'm going to try to wait zero the BIOS that makes the counter

428
00:26:55,360 --> 00:26:57,152
Kendall Ryan

429
00:27:02,528 --> 00:27:06,880
Next will be very WW2 the third of those weights

430
00:27:07,648 --> 00:27:08,928
What makes the consoles

431
00:27:09,440 --> 00:27:10,976
Wonderland black vests

432
00:27:13,024 --> 00:27:15,584
And finally Albury wait one

433
00:27:15,840 --> 00:27:20,704
Which will make the whole thing pivot around the place where the red line intersects the vertical axis then

434
00:27:22,240 --> 00:27:23,008
All right

435
00:27:29,408 --> 00:27:35,552
Okay and what I'm going to do next is very all three of them so I'm going to scale

436
00:27:35,808 --> 00:27:40,160
Lights up or down by a factor and I want to check that you're with me

437
00:27:40,416 --> 00:27:46,560
What's going to happen if I double all three of the weights of this neuron what's going to happen to the Contour

438
00:27:48,864 --> 00:27:49,888
Chatuenate

439
00:27:51,680 --> 00:27:52,704
Hopefully this is all

440
00:27:53,472 --> 00:27:54,496
Are straightforward

441
00:27:59,872 --> 00:28:04,992
Okay any prediction of what happens if I take all three weights in Dublin

442
00:28:07,296 --> 00:28:08,064
Anyone

443
00:28:11,136 --> 00:28:14,208
Stipa and where does the red line go

444
00:28:14,976 --> 00:28:15,488
Moose

445
00:28:16,000 --> 00:28:16,512
Stay still

446
00:28:18,304 --> 00:28:20,864
It stay still

447
00:28:21,120 --> 00:28:24,704
It just becomes speaker so let's go back to the

448
00:28:24,960 --> 00:28:25,984
Then I

449
00:28:31,360 --> 00:28:37,504
So scaling up and down the weights doesn't change the place where the activation is zero

450
00:28:38,528 --> 00:28:39,808
Which is where the red line

451
00:28:40,576 --> 00:28:41,088
It's

452
00:28:42,112 --> 00:28:44,928
Okay so that was playing with a single

453
00:28:45,184 --> 00:28:45,696
Neuron

454
00:28:46,464 --> 00:28:52,608
Know what we're going to do next is really sore with a neural network consisting of just one liver on

455
00:28:52,864 --> 00:28:55,680
Learning means that ye something data

456
00:29:01,568 --> 00:29:05,920
On the data takes the form of a set of input

457
00:29:06,176 --> 00:29:07,968
Target

458
00:29:08,224 --> 00:29:14,368
Passed so Target specifies roughly what you want

459
00:29:14,624 --> 00:29:17,696
The neuron to say or the neural network to say

460
00:29:18,720 --> 00:29:19,488
In response

461
00:29:19,744 --> 00:29:21,024
To the input

462
00:29:21,536 --> 00:29:25,376
And I'll have a label cold and run through

463
00:29:25,632 --> 00:29:26,400
Please

464
00:29:27,424 --> 00:29:27,936
Basa

465
00:29:28,192 --> 00:29:31,520
And the idea of living is that we adjust

466
00:29:32,032 --> 00:29:34,848
Learning is just a silly name for adjusting

467
00:29:36,384 --> 00:29:37,920
The promises

468
00:29:39,968 --> 00:29:42,016
Weights

469
00:29:43,040 --> 00:29:44,320
Search that

470
00:29:44,832 --> 00:29:50,976
Output of your neural network why when you shove in xn

471
00:29:52,256 --> 00:29:54,816
Is closed

472
00:29:59,936 --> 00:30:00,704
Surolan

473
00:30:07,104 --> 00:30:09,920
Obviously when someone says make

474
00:30:10,176 --> 00:30:11,712
Something like that happen

475
00:30:11,968 --> 00:30:18,112
What you would say just tell me that please and then I'll minimize it so I'll do it that way

476
00:30:18,368 --> 00:30:19,136
Function

477
00:30:19,392 --> 00:30:21,952
Which measures how good the wait saw

478
00:30:23,232 --> 00:30:24,256
Okogie

479
00:30:26,304 --> 00:30:31,424
To start off with a G is going to have enzymes Each of which measures how close t

480
00:30:31,680 --> 00:30:34,496
Is 2 why am I measure closeness is going to be

481
00:30:34,752 --> 00:30:37,056
A rose event fee

482
00:30:56,768 --> 00:30:58,048
Okay

483
00:30:58,304 --> 00:30:58,816
Where

484
00:30:59,584 --> 00:31:05,728
Why and is my shorthand for why when you come in at 10 and have the light

485
00:31:07,264 --> 00:31:10,592
Okay 7 W dependents looks in y

486
00:31:11,360 --> 00:31:12,384
Nnsy

487
00:31:13,408 --> 00:31:14,176
And

488
00:31:14,944 --> 00:31:18,016
This objective function is a sum of terms

489
00:31:21,344 --> 00:31:23,136
Beechwood either looks like this

490
00:31:23,648 --> 00:31:24,672
Borat looks like

491
00:31:25,184 --> 00:31:27,488
This depending whether the target

492
00:31:27,744 --> 00:31:29,024
Is there a

493
00:31:29,280 --> 00:31:30,560
All the target

494
00:31:30,816 --> 00:31:31,328
Islam

495
00:31:31,840 --> 00:31:32,352
So

496
00:31:32,608 --> 00:31:33,888
If y matches the target

497
00:31:34,144 --> 00:31:35,936
The objective function give you the Rock

498
00:31:36,704 --> 00:31:41,824
NF why doesn't match the target you got the penalty that's increasingly big

499
00:31:42,592 --> 00:31:44,896
You could think of G as being

500
00:31:46,944 --> 00:31:49,248
The information content of brighter

501
00:31:53,344 --> 00:31:59,488
From the point of view of the neuron who views his wife as being the correct

502
00:31:59,744 --> 00:32:01,536
Kentucky distribution for the data

503
00:32:03,584 --> 00:32:06,144
Sorry it's the information content

504
00:32:06,400 --> 00:32:10,496
I'll be Tracer or currently it is the description length

505
00:32:15,360 --> 00:32:16,384
All of the data

506
00:32:22,272 --> 00:32:23,040
Assuming

507
00:32:24,320 --> 00:32:25,600
But you view why

508
00:32:25,856 --> 00:32:26,624
As being

509
00:32:27,904 --> 00:32:28,928
The correct

510
00:32:32,000 --> 00:32:32,768
Pravasi

511
00:32:34,816 --> 00:32:36,352
40

512
00:32:36,864 --> 00:32:40,448
Now when I said good day sir I'm being a bit imprecise what I mean is

513
00:32:41,472 --> 00:32:45,312
The set of cheese

514
00:32:45,568 --> 00:32:50,176
Text you if you've already been told where the inputs are what the exes are

515
00:32:50,432 --> 00:32:52,480
And then Simon Says now I'm going to tell you that he's

516
00:32:52,736 --> 00:32:57,344
This is what the description length would be if you used a perfect asthmatic Kota

517
00:32:58,112 --> 00:33:01,952
Or other compression message to encode them using this new wrong

518
00:33:02,208 --> 00:33:04,768
Okay so

519
00:33:09,888 --> 00:33:12,192
Let's Show an example of this

520
00:33:13,984 --> 00:33:16,800
So is a single neuron and has some data

521
00:33:17,312 --> 00:33:22,432
The sort of data we can be talking about might be we are trying to run a vegetable salting Factory

522
00:33:22,944 --> 00:33:29,088
The pigeons are going on strike because they realize how good they are compared to a super computer and they want to be paid the same wage

523
00:33:29,344 --> 00:33:30,368
Images of supercomputers

524
00:33:30,624 --> 00:33:31,904
So now we're trying to

525
00:33:33,184 --> 00:33:39,328
A simple neuron to replace the pigeon pigeon is to sort the potatoes from the carrots or the razor

526
00:33:39,584 --> 00:33:40,608
News from the Pebbles

527
00:33:40,864 --> 00:33:41,632
What are we doing

528
00:33:41,888 --> 00:33:46,752
Labeled Asia so we measured X1 and X2 for 10 objects

529
00:33:47,264 --> 00:33:50,080
Five of them with raisins and 5 with pedals

530
00:33:50,336 --> 00:33:51,872
Okay so the class is off

531
00:33:52,128 --> 00:33:53,152
Color that

532
00:33:53,408 --> 00:33:56,480
So we know what class each of the 10 things it in

533
00:33:56,992 --> 00:33:58,528
X1 X2

534
00:33:58,784 --> 00:34:04,928
And what I showed you in this input space that connects to is how a neuron can produce a function

535
00:34:05,440 --> 00:34:07,232
Is a ramp

536
00:34:07,488 --> 00:34:10,048
A soft sigmoid ramp

537
00:34:11,072 --> 00:34:13,120
So what did learning learning is

538
00:34:13,376 --> 00:34:16,704
Minimizing an objective function like this with respect to w

539
00:34:18,496 --> 00:34:24,640
So what does that involve well if you're at all sensible at minimizing things you'll say

540
00:34:24,896 --> 00:34:27,712
Is it easy to compute the gradient

541
00:34:27,968 --> 00:34:31,296
And the answer is yes it's easy to compute the gradient

542
00:34:31,552 --> 00:34:32,832
So you got the gradient

543
00:34:33,088 --> 00:34:37,952
I Look a Lot G which is d g e w

544
00:34:38,464 --> 00:34:41,536
And then you do something sensible downhill

545
00:34:41,792 --> 00:34:43,072
Thing based on the gradient

546
00:34:43,328 --> 00:34:47,424
The very simplest thing you could do with a gradient is cool gradient descend

547
00:34:47,680 --> 00:34:52,288
That's not a sensible thing to do because it doesn't satisfy

548
00:34:52,800 --> 00:34:56,896
Covariance that you'd like the appropriate treatment of

549
00:34:57,408 --> 00:35:03,552
Vector spaces I'm that do so this mixes up Vector spaces and The Jewels which is a crime

550
00:35:03,808 --> 00:35:05,088
But lots of people do it

551
00:35:05,600 --> 00:35:07,392
So you just look out the gradient

552
00:35:07,648 --> 00:35:10,208
Mini-site let's make a step in the direction of the gradient

553
00:35:10,720 --> 00:35:12,256
So that's gradient descent

554
00:35:13,024 --> 00:35:14,048
And this

555
00:35:14,304 --> 00:35:15,328
Promises hair

556
00:35:15,840 --> 00:35:19,424
Is binaural Network people cold the letting rate

557
00:35:21,728 --> 00:35:24,288
I'm not going to talk, so that you have to set somehow

558
00:35:25,312 --> 00:35:27,360
It's fairly easy to

559
00:35:27,616 --> 00:35:31,456
Show on this is left as a homework exercise

560
00:35:31,712 --> 00:35:34,784
That the gradient can be written as a sum over all of the

561
00:35:35,040 --> 00:35:39,648
Examples of t - y * X

562
00:35:41,696 --> 00:35:46,560
You can think of this as the era that the neuron is currently making on the ends example

563
00:35:46,816 --> 00:35:48,096
X the input

564
00:35:48,352 --> 00:35:51,680
Soap to work out the gradient you put each input in

565
00:35:51,936 --> 00:35:54,752
You look at the area between the output in the target

566
00:35:55,008 --> 00:35:58,592
Fenty X Arrow by the input to get the gradient and you toss it up

567
00:35:58,848 --> 00:35:59,616
And that

568
00:35:59,872 --> 00:36:04,480
Simple operation which we got by differentiation

569
00:36:04,736 --> 00:36:09,856
Is sometimes called backpropagation the backpropagation is the neural networks community

570
00:36:10,112 --> 00:36:13,952
The neural network communities name for differentiation

571
00:36:20,352 --> 00:36:24,960
Backpropagation is a particular algorithm for doing differentiation in Freeport

572
00:36:25,472 --> 00:36:27,776
Networks when doing it efficiently

573
00:36:29,312 --> 00:36:32,896
Okay so what happens if we do that we start off the new alarm

574
00:36:33,664 --> 00:36:35,456
With some randomly chosen weights

575
00:36:35,712 --> 00:36:41,856
And the weights change so hit is grating just can't do anything and I'm showing you the three weight values

576
00:36:42,112 --> 00:36:43,136
Bull blow up

577
00:36:43,648 --> 00:36:49,792
What was happening in WW1 WW2 space looks like this the weights one that off in One Direction and then they want to know

578
00:36:50,048 --> 00:36:54,400
Directions to downhill downhill changed

579
00:36:54,656 --> 00:37:00,800
What was getting on well his the data is the initial weight back to the life I picked at random

580
00:37:01,312 --> 00:37:06,944
So it's not doing a very good job of separating the blue from the yellow and you do gradient descent

581
00:37:07,200 --> 00:37:07,968
And

582
00:37:08,480 --> 00:37:13,344
The weight change on where the red line is changes and it rotates around

583
00:37:13,600 --> 00:37:15,904
And then it changes and it rotates

584
00:37:16,416 --> 00:37:17,696
I ran some more

585
00:37:17,952 --> 00:37:20,256
And the way to getting bigger and bigger

586
00:37:22,048 --> 00:37:24,864
And after 40,000 attractions that's where the weights of

587
00:37:25,120 --> 00:37:25,632
Coptic

588
00:37:26,400 --> 00:37:27,168
All right

589
00:37:27,680 --> 00:37:28,704
So that's learning

590
00:37:28,960 --> 00:37:30,496
Anyone say well

591
00:37:31,008 --> 00:37:35,616
I'm not happy maybe I can do better than that because

592
00:37:36,384 --> 00:37:41,248
Al moment ago before we got to this final optimized

593
00:37:41,504 --> 00:37:44,064
Managed to find a place where I can put a cliff

594
00:37:44,320 --> 00:37:46,880
The completely separate the other points from the blue Plains

595
00:37:47,392 --> 00:37:53,536
A moment before when we was little halfway that the answer looked a bit more reasonable this is it this is now

596
00:37:53,792 --> 00:37:59,936
Another example of a classified for you arrived Hillside 99.9

597
00:38:00,192 --> 00:38:01,216
Sure it's a blue

598
00:38:01,472 --> 00:38:04,544
But it's a yellow because it's incredibly steep cliff

599
00:38:05,056 --> 00:38:06,592
Stupid

600
00:38:06,848 --> 00:38:09,152
We don't like this outcome

601
00:38:09,408 --> 00:38:10,944
Has managed to

602
00:38:11,200 --> 00:38:14,272
Minimize objective function but maybe we picked the wrong objective function

603
00:38:15,296 --> 00:38:16,064
So

604
00:38:16,576 --> 00:38:19,648
If we don't like this what we do will sensible people

605
00:38:19,904 --> 00:38:21,184
Change function

606
00:38:24,256 --> 00:38:26,560
So now what we're going to do is learning

607
00:38:31,168 --> 00:38:33,984
With

608
00:38:34,240 --> 00:38:37,824
The regularisation which

609
00:38:40,896 --> 00:38:41,408
As

610
00:38:41,664 --> 00:38:43,200
I don't like these enormous

611
00:38:43,456 --> 00:38:44,992
Steep shop

612
00:38:45,248 --> 00:38:51,392
Change changes in function or Enduro that what language we wait because because you got State functions

613
00:38:53,696 --> 00:38:54,720
When the waves are big

614
00:38:56,000 --> 00:38:56,768
So

615
00:38:57,024 --> 00:38:58,304
We added an extra turn

616
00:38:58,560 --> 00:39:04,704
In the objective function to get change so it's not just G the new objective function

617
00:39:05,472 --> 00:39:07,776
Letter G plus an extra time

618
00:39:09,056 --> 00:39:10,080
Rick Sanchez

619
00:39:11,872 --> 00:39:14,176
Others like big weights

620
00:39:14,432 --> 00:39:18,016
Subjectivist change from please put the diapers all the possible to

621
00:39:18,272 --> 00:39:22,112
Please put the right up but I don a penalty is for having big weights

622
00:39:22,368 --> 00:39:24,160
Ew

623
00:39:24,416 --> 00:39:28,256
Is going to be defined to be

624
00:39:28,512 --> 00:39:30,048
Sum of w

625
00:39:30,304 --> 00:39:31,840
K scratch

626
00:39:33,632 --> 00:39:34,144
All right

627
00:39:36,192 --> 00:39:38,496
Right so we do that

628
00:39:41,824 --> 00:39:45,152
Not his what happens I've set this parameter Alpha

629
00:39:46,688 --> 00:39:48,480
Which is either cold

630
00:39:48,736 --> 00:39:52,064
The regularizer if you are from the statistics world

631
00:39:52,576 --> 00:39:54,880
Oh sorry the regularisation Clemson

632
00:39:58,720 --> 00:40:02,048
Ew is cold but regular

633
00:40:03,840 --> 00:40:07,424
Or if you're in neural networks Alfred's cold the way she tell you right

634
00:40:13,824 --> 00:40:19,200
When you have their own what happens to your gradient well it's just an extra ounce of the gradient of

635
00:40:19,456 --> 00:40:22,272
Cheapest Alfa ew is this Loft Plus

636
00:40:22,528 --> 00:40:24,064
Alpha x your weights

637
00:40:24,576 --> 00:40:28,160
So you're going downhill on that means you've got to minus Alpha

638
00:40:28,416 --> 00:40:29,184
W

639
00:40:29,696 --> 00:40:30,464
Tom and

640
00:40:30,720 --> 00:40:35,840
What happens when you do the minimisation is showing here in terms of what the weights do they used to blow up

641
00:40:36,096 --> 00:40:37,376
But now I settle down

642
00:40:37,632 --> 00:40:38,656
And they don't

643
00:40:38,912 --> 00:40:40,704
We don't have a nevus deepening

644
00:40:40,960 --> 00:40:41,472
Function

645
00:40:41,984 --> 00:40:48,128
So wait to settle down and his what used to happen when we didn't have the regular rise of the orange color

646
00:40:48,384 --> 00:40:49,920
Blowing up

647
00:40:50,944 --> 00:40:57,088
Did what happens in WW1 WW2 space we go downhill downhill enso to stop along the trajectory

648
00:40:57,344 --> 00:41:03,488
Great if I could lay is what happens so we follow message saying trajectory and weight space but we stopped

649
00:41:03,744 --> 00:41:06,304
I'm at an Optimum this new objective function

650
00:41:06,560 --> 00:41:10,912
This is why we used to be yelling at the 40,000 decorations of steepest descent

651
00:41:11,168 --> 00:41:17,312
Not when we switch on the way Decay we go downhill and we end up in this place hair

652
00:41:17,824 --> 00:41:20,128
Josh White resigns this again so it looks

653
00:41:20,896 --> 00:41:21,664
Sensible

654
00:41:23,456 --> 00:41:26,784
All right and you might look good

655
00:41:29,344 --> 00:41:35,488
Because now it's not making completely unreasonable overconfident answers for example if you say for another Point arriving at

656
00:41:36,256 --> 00:41:37,280
Location hair

657
00:41:38,560 --> 00:41:44,448
What's the chance of is in the blue class it'll say about 90% instead of saying I'm a hundred percent sure

658
00:41:45,728 --> 00:41:46,496
And

659
00:41:46,752 --> 00:41:47,264
If you're Dutch

660
00:41:47,776 --> 00:41:50,848
Another one of those yellow White Castle safe if you could see

661
00:41:53,407 --> 00:41:54,431
Based on

662
00:41:54,943 --> 00:41:57,503
Potato Head balloon raisins

663
00:41:58,271 --> 00:42:03,391
You might still be dissatisfied and if you are hold that thought will come back to this in a moment

664
00:42:04,671 --> 00:42:06,975
Not we're going to switch to

665
00:42:07,999 --> 00:42:09,535
Lava Networks

666
00:42:09,791 --> 00:42:13,631
And then we'll come back to the the the single neuron when we fought some more ideas

667
00:42:14,655 --> 00:42:15,679
Let me just

668
00:42:16,959 --> 00:42:23,103
Couple more things about the single neuron and then we'll come back to that this learning business

669
00:42:25,407 --> 00:42:31,551
First I can give you an example of using a single neuron to actually do a real problem so I told you about

670
00:42:31,807 --> 00:42:37,951
Golden raisins while here inside is a handwriting problem you can get yourself a dataset of handwritten digit

671
00:42:38,207 --> 00:42:40,767
They're being digitized into black and white dots

672
00:42:42,047 --> 00:42:45,887
In 256 Dimensions so you can get yourself a few thousand

673
00:42:46,399 --> 00:42:48,447
Examples of twos and threes

674
00:42:49,471 --> 00:42:49,983
And

675
00:42:50,239 --> 00:42:52,799
Use the algorithm I just described

676
00:42:53,055 --> 00:42:58,175
To come up with a single neuron which has 256 input has 250

677
00:42:58,431 --> 00:42:58,943
7

678
00:42:59,199 --> 00:42:59,711
Wait

679
00:42:59,967 --> 00:43:06,111
And its output can be read as the probability that this thing I'm looking at is at 2 or 3 when you do that this is what the weights

680
00:43:06,367 --> 00:43:07,135
Optimized

681
00:43:07,391 --> 00:43:08,159
Look like

682
00:43:08,671 --> 00:43:10,975
And the error rate of the classifier that you

683
00:43:11,231 --> 00:43:13,791
I've made is about 10% so on this

684
00:43:14,047 --> 00:43:20,191
Database of handwritten digits it gets completely useless even a single

685
00:43:20,447 --> 00:43:24,031
Iran can be an interesting quick and dirty way to solve

686
00:43:24,287 --> 00:43:25,055
Simpson pool

687
00:43:25,567 --> 00:43:26,591
Inference problems

688
00:43:29,151 --> 00:43:31,711
A question you could ask about a single neuron

689
00:43:32,479 --> 00:43:33,247
Is

690
00:43:33,503 --> 00:43:35,807
You can do as a communication Channel

691
00:43:36,319 --> 00:43:37,599
So

692
00:43:38,879 --> 00:43:43,231
This is just a little aside and there's a whole chapter on this in the book if you're interested

693
00:43:44,255 --> 00:43:46,559
One way of thinking about what's going on here is

694
00:43:46,815 --> 00:43:52,959
Oh I don't really believe that the output of the neuron is the correct from Realty distribution so forget that idea

695
00:43:53,215 --> 00:43:56,543
But I do believe that the neuron is a helpful way to package up

696
00:43:57,055 --> 00:43:58,591
The contents of the data set

697
00:43:59,359 --> 00:44:01,663
So if you give me a date to that

698
00:44:02,431 --> 00:44:04,735
With lots of lots of examples of handwritten digits

699
00:44:04,991 --> 00:44:06,783
I can package all of that up

700
00:44:07,039 --> 00:44:09,599
Into 257 middle numbers

701
00:44:10,623 --> 00:44:12,415
What y'all say a single neuron

702
00:44:12,671 --> 00:44:16,511
And then I can send the neuron to someone so I can send the weights w

703
00:44:17,023 --> 00:44:18,559
And then someone else

704
00:44:18,815 --> 00:44:21,631
Could try and reconstruct what the labels were

705
00:44:22,399 --> 00:44:22,911
Bye

706
00:44:23,423 --> 00:44:25,215
I'm assuming I can get the same inputs

707
00:44:25,727 --> 00:44:28,543
When I get those inputs they can use the neuron to reconstruct

708
00:44:29,055 --> 00:44:30,335
What the output was

709
00:44:38,783 --> 00:44:41,087
So is this sort of information Theory

710
00:44:41,343 --> 00:44:42,367
If you of

711
00:44:42,879 --> 00:44:43,903
I think we are on

712
00:44:44,415 --> 00:44:45,951
And you can ask the question

713
00:44:46,463 --> 00:44:48,511
If I view the neuron as a channel

714
00:44:50,047 --> 00:44:51,583
What would capacity of that channel

715
00:44:52,095 --> 00:44:54,655
How many bits calendar on love

716
00:44:56,447 --> 00:44:58,495
And you can pull my side question

717
00:44:59,007 --> 00:45:01,055
Find the chapter in the book does this

718
00:45:01,823 --> 00:45:04,639
And has a rather interesting non-trivial answer

719
00:45:05,663 --> 00:45:10,015
Which comes out like this assuming that these exes are in random

720
00:45:10,271 --> 00:45:12,319
The are in general position so bad

721
00:45:12,575 --> 00:45:17,183
Either randomly distributed oil at least enough nastaliq alinea with each other

722
00:45:17,439 --> 00:45:19,487
Taxes are in general position

723
00:45:19,743 --> 00:45:21,535
Then there's a good chance

724
00:45:21,791 --> 00:45:22,303
That

725
00:45:23,071 --> 00:45:25,887
You can correctly learn with your neuron

726
00:45:28,191 --> 00:45:28,959
Any

727
00:45:29,215 --> 00:45:30,751
Random labeling

728
00:45:31,775 --> 00:45:33,055
The world comes up with

729
00:45:34,335 --> 00:45:35,871
Has a good chance you can learn that

730
00:45:36,383 --> 00:45:38,687
As long as the number of labels

731
00:45:38,943 --> 00:45:39,455
And

732
00:45:40,479 --> 00:45:41,759
Is less than or equal to

733
00:45:42,015 --> 00:45:43,551
Twice the number of inputs

734
00:45:44,063 --> 00:45:45,855
Which is twice the number promises

735
00:45:46,623 --> 00:45:47,135
So

736
00:45:47,391 --> 00:45:52,511
What did I just say I said a single neuron with cayenne puts can almost certain they memorize

737
00:45:53,023 --> 00:45:53,791
2K

738
00:45:54,303 --> 00:45:55,583
Random binary labels

739
00:45:56,095 --> 00:45:58,911
That means that the capacity of a neuron in terms of

740
00:45:59,167 --> 00:46:04,031
How many bits you can stall and it's weights each of the weights only one up 2wk

741
00:46:04,287 --> 00:46:05,823
Each of them is a real number

742
00:46:06,591 --> 00:46:10,943
But effectively if you're using the neuron in this way as a way of Justice

743
00:46:11,199 --> 00:46:14,527
Learning the labels on some examples and then reproducing them

744
00:46:15,039 --> 00:46:16,575
You can reliably communicate

745
00:46:16,831 --> 00:46:18,111
With that channel

746
00:46:18,367 --> 00:46:19,135
Two bits

747
00:46:19,391 --> 00:46:20,159
Parent Connection

748
00:46:20,415 --> 00:46:23,231
That's a really handy rule of thumb

749
00:46:23,487 --> 00:46:24,511
The many

750
00:46:24,767 --> 00:46:25,535
Neural net

751
00:46:25,791 --> 00:46:27,327
Style application

752
00:46:27,583 --> 00:46:29,119
If you're using a neural net

753
00:46:29,631 --> 00:46:33,727
Then maybe the effective number of bits you can learn in that

754
00:46:33,983 --> 00:46:35,775
It's something like twice

755
00:46:36,031 --> 00:46:38,847
The number of promises you've gotten in the network

756
00:46:39,359 --> 00:46:45,503
This is precisely true for a single neuron and it may be true for all the types of neural network as well but you need to

757
00:46:45,759 --> 00:46:47,295
Use that idea with cat

758
00:46:48,831 --> 00:46:53,695
Fright not letting us communication under the capacity results for a single neuron

759
00:46:55,743 --> 00:47:00,863
What I want to do now is move to slightly more realistic networks about bigger

760
00:47:02,143 --> 00:47:02,911
And

761
00:47:03,423 --> 00:47:06,495
I'm going to show you a neural network which has

762
00:47:07,007 --> 00:47:07,775
A load of

763
00:47:08,287 --> 00:47:09,823
What we call Hidden neurons

764
00:47:11,871 --> 00:47:13,663
Simplicity I'll show you

765
00:47:14,175 --> 00:47:18,015
A network with just one input which is where the mouse is pointing now

766
00:47:18,527 --> 00:47:20,831
This is the bias unit if you like

767
00:47:21,087 --> 00:47:22,879
And this is the output

768
00:47:23,135 --> 00:47:26,207
So it's a multi-layer network with one hidden layer

769
00:47:26,463 --> 00:47:27,743
Hugo from the input

770
00:47:29,279 --> 00:47:30,047
Where are we

771
00:47:31,071 --> 00:47:35,167
You go from the input which hair is just one dimensional

772
00:47:35,679 --> 00:47:38,495
Neurons

773
00:47:41,311 --> 00:47:47,455
And then you got to your output hair haven't gone through all of these guys here

774
00:47:47,711 --> 00:47:48,735
Is the number of hidden

775
00:47:48,991 --> 00:47:49,759
Neurons

776
00:47:50,015 --> 00:47:51,807
The number of parameters in this thing

777
00:47:53,343 --> 00:47:55,135
Is roughly three

778
00:47:56,159 --> 00:47:56,927
H

779
00:47:59,743 --> 00:48:03,071
Because this to input switch one and then this guy hair

780
00:48:03,327 --> 00:48:08,703
Has roughly H Prime has going to go more actually 3/8 + 1

781
00:48:09,983 --> 00:48:11,775
Okay so I want to

782
00:48:12,031 --> 00:48:18,175
Explore bit more what functions look like for a neural network like this this is the simplest exam

783
00:48:18,431 --> 00:48:19,455
Allpay

784
00:48:19,711 --> 00:48:21,759
Multi-layer perceptron

785
00:48:28,415 --> 00:48:29,695
Wigwam hidden lair

786
00:48:30,975 --> 00:48:35,583
Liesl mobile apps that parents are still quite widely used

787
00:48:35,839 --> 00:48:36,863
Interesting to

788
00:48:37,119 --> 00:48:37,887
Understanding

789
00:48:38,399 --> 00:48:40,191
Turn light switch to Yellow demo

790
00:48:42,751 --> 00:48:46,335
What I'm doing here is I'm going to show you the case h = 3

791
00:48:46,591 --> 00:48:47,615
I'm going to make

792
00:48:47,871 --> 00:48:48,895
A random

793
00:48:49,151 --> 00:48:55,295
Network unlock Sim setting all of the weights how many of the 6/10 I'm setting the 10 weights of

794
00:48:55,551 --> 00:48:56,063
This network

795
00:48:56,319 --> 00:49:02,463
The random values drawn from a passing distribution and then I'll update those wait and we'll see what happens to the the Run

796
00:49:02,719 --> 00:49:03,999
Even function

797
00:49:04,255 --> 00:49:06,559
That's a whole bunch of functions you can get

798
00:49:07,071 --> 00:49:08,351
Functions of the

799
00:49:08,607 --> 00:49:14,751
Input variable which is showing you the input variable X warm on the horizontal axis in the bus

800
00:49:15,007 --> 00:49:16,543
Texas is the output y

801
00:49:16,799 --> 00:49:18,079
Does coming out help this

802
00:49:18,335 --> 00:49:19,103
Multi-layered Network

803
00:49:19,615 --> 00:49:20,127
Right

804
00:49:20,895 --> 00:49:25,759
So that was a little movie of a bunch of functions what are you doing we're just adding up three tentious

805
00:49:26,015 --> 00:49:27,807
It's the sum of 3 hyperbolic tangent

806
00:49:28,575 --> 00:49:34,719
And that's what the blue thing is this an example of what you got when you make a weighted sum of three Alba tree

807
00:49:34,975 --> 00:49:35,999
Tangent functions

808
00:49:36,511 --> 00:49:42,399
Another name for most neural networks that use. In the real world is

809
00:49:42,655 --> 00:49:44,959
Weighted sums of hyperbolic tangent

810
00:49:45,215 --> 00:49:47,263
That doesn't sound quite so sexy

811
00:49:48,543 --> 00:49:54,687
Okay his only 10 hidden neurons and the blue line is a some of those 10 hyperbolic tangent

812
00:49:54,943 --> 00:50:00,063
All the things that go between -1 + 1 of hyperbolic tangent functions themselves that the

813
00:50:00,319 --> 00:50:01,855
10 neurons of computing

814
00:50:02,367 --> 00:50:05,439
Okay we randomize the weight Seneca is very busy with

815
00:50:05,695 --> 00:50:09,023
And that shows you the sort of thing that you got for a single input

816
00:50:09,279 --> 00:50:10,047
Single output

817
00:50:10,303 --> 00:50:11,071
Neural network

818
00:50:11,327 --> 00:50:14,399
That's increase 225 and has a whole bunch of random functions

819
00:50:15,167 --> 00:50:16,191
So what's it look like

820
00:50:16,447 --> 00:50:20,287
It looks like a French cuff and so another way of thinking about your neural network is

821
00:50:20,543 --> 00:50:21,823
Please feed forward

822
00:50:22,079 --> 00:50:24,639
Multi-line not but I'll just French cuffs

823
00:50:25,151 --> 00:50:28,223
And when you then bring along some data inside please fit

824
00:50:28,479 --> 00:50:30,527
This girl at work today so you're just saying

825
00:50:30,783 --> 00:50:32,319
Please put a French cuff

826
00:50:32,831 --> 00:50:35,647
Through the data you know what I mean by friends cuz it's one of those

827
00:50:35,903 --> 00:50:38,975
Bendy things you buy something

828
00:50:39,231 --> 00:50:39,999
What's up

829
00:50:40,255 --> 00:50:41,535
Bendy and rubbery

830
00:50:41,791 --> 00:50:44,351
And it gives you a smooth curve Precinct

831
00:50:44,607 --> 00:50:47,423
I don't know maybe they don't sell them anymore

832
00:50:47,679 --> 00:50:51,263
But that used to be popular in in in high school so that you could

833
00:50:51,519 --> 00:50:53,055
Make a bendy smooth

834
00:50:53,567 --> 00:50:54,079
Smooth cuff

835
00:50:55,615 --> 00:50:56,383
Right

836
00:50:56,639 --> 00:50:58,943
So what can we do next

837
00:51:00,223 --> 00:51:03,551
The devil I just showed you with 25 all hidden units

838
00:51:03,807 --> 00:51:08,671
I drew the weights at random from a gaussian distribution I didn't tell you what the standard deviations with

839
00:51:08,927 --> 00:51:12,767
Now I'm going to show you those standard deviations they were all equal a moment ago

840
00:51:13,023 --> 00:51:18,399
And the screen is now showing you for equal standard deviations for the input weights

841
00:51:18,911 --> 00:51:19,935
The bias weights

842
00:51:20,191 --> 00:51:25,311
How many outputs which are colored Now red

843
00:51:25,567 --> 00:51:26,591
Purple

844
00:51:28,127 --> 00:51:29,407
And

845
00:51:29,663 --> 00:51:30,687
Gray

846
00:51:35,807 --> 00:51:39,391
Barre3 blue cuz I'll show you three random draws

847
00:51:39,903 --> 00:51:40,415
From

848
00:51:42,207 --> 00:51:43,231
Bhagat Singh distribution

849
00:51:43,487 --> 00:51:45,535
Right so you can see those three functions

850
00:51:45,791 --> 00:51:49,375
I know what I'm going to do is crank up and down the standard deviations just so you can see

851
00:51:49,631 --> 00:51:53,215
What effect scaling the white survives the running weights

852
00:51:53,471 --> 00:51:57,567
Making bigger or smaller has on the random functions so first

853
00:51:57,823 --> 00:52:01,151
Let's crank up the input weights standard deviation

854
00:52:01,407 --> 00:52:02,687
We see the functions of

855
00:52:02,943 --> 00:52:09,087
Moore weekly shrunken hyperbolic tangent

856
00:52:10,623 --> 00:52:11,903
Okay if we take

857
00:52:12,415 --> 00:52:13,439
Sigma in

858
00:52:13,695 --> 00:52:19,839
The standard deviation of all the input lights down broad functions that locally

859
00:52:20,095 --> 00:52:21,119
You look just like straight line

860
00:52:22,655 --> 00:52:23,423
Okay

861
00:52:23,679 --> 00:52:26,751
Now let's treat the biases biases we can

862
00:52:27,007 --> 00:52:31,103
Have big standard deviation for them

863
00:52:31,359 --> 00:52:32,127
We can have

864
00:52:32,383 --> 00:52:38,527
Smooth on adoration and now we got something that looks like it's a sum of about three or four hyperbolic tangent even though it's a function

865
00:52:38,783 --> 00:52:39,551
25

866
00:52:40,319 --> 00:52:45,439
So the standard deviation of the biases is 217 how many wiggles you get

867
00:52:45,951 --> 00:52:46,975
Hobbit it doesn't affect

868
00:52:47,231 --> 00:52:50,815
The characteristic length scale of The Wiggles that was being controlled by

869
00:52:51,071 --> 00:52:52,607
The standard deviation of the inputs

870
00:52:53,375 --> 00:52:57,471
Finally free prank up and down the standard deviation of the outputs this is trivial

871
00:52:57,727 --> 00:52:59,263
The functions just become

872
00:52:59,519 --> 00:53:01,311
Hiya vertically

873
00:53:01,567 --> 00:53:02,335
Hola

874
00:53:02,847 --> 00:53:03,359
All right

875
00:53:03,871 --> 00:53:06,943
So that's the role of those three standard deviations

876
00:53:08,991 --> 00:53:10,783
So what do we do next

877
00:53:11,039 --> 00:53:12,831
Let's go sell some data

878
00:53:13,343 --> 00:53:16,159
So just as with a single neuron

879
00:53:16,671 --> 00:53:17,183
We

880
00:53:17,439 --> 00:53:20,511
Compute the gradient steepest descent on that grade in

881
00:53:21,023 --> 00:53:21,791
And we ended up

882
00:53:22,303 --> 00:53:27,679
We have this perfect you separate the five yellow petals from the five blue raisins

883
00:53:28,191 --> 00:53:32,287
We can do the same thing with the basis that looks like that's here off 5 beta

884
00:53:32,543 --> 00:53:33,055
Point

885
00:53:33,823 --> 00:53:37,663
And we can define a new objective function which instead of

886
00:53:39,199 --> 00:53:40,735
Being with geha

887
00:53:44,063 --> 00:53:46,111
It's going to be at Manchester real numbers

888
00:53:48,927 --> 00:53:49,951
Apocalypse

889
00:53:53,023 --> 00:53:55,583
The outputs onto targets so we have

890
00:53:55,839 --> 00:53:58,143
A girl that function like this cold why

891
00:53:58,399 --> 00:54:00,959
You have some Target values like this

892
00:54:01,215 --> 00:54:03,775
These are the errors

893
00:54:06,847 --> 00:54:09,663
Christian is William

894
00:54:10,175 --> 00:54:14,015
American Define EDD equals sum of

895
00:54:14,271 --> 00:54:14,783
Ha

896
00:54:16,319 --> 00:54:18,623
TN - y n

897
00:54:18,879 --> 00:54:19,903
Square

898
00:54:20,927 --> 00:54:23,743
What other measure of error could you possibly have well

899
00:54:23,999 --> 00:54:27,583
Obviously other ones that this is a standard thing people often some squares

900
00:54:28,351 --> 00:54:31,423
And that depends on W on the weights

901
00:54:31,935 --> 00:54:35,007
Through the white ends which are old

902
00:54:35,263 --> 00:54:36,031
Weight dependent

903
00:54:37,567 --> 00:54:38,079
Right

904
00:54:39,103 --> 00:54:44,223
So what do we do with that we say this is our objective function please make the output of Twisted puppet all the targets

905
00:54:44,479 --> 00:54:48,575
You compute the gradient G which by definition is DED

906
00:54:48,831 --> 00:54:53,695
By the W and many people do steepest descent I'm not objective

907
00:54:53,951 --> 00:54:55,743
Function set a gradient downhill

908
00:54:56,511 --> 00:54:58,303
All right lights on please

909
00:54:59,583 --> 00:55:05,727
And his what happens after one direction of steepest descent so I started off with 25 hidden units

910
00:55:05,983 --> 00:55:07,775
Sharon by the blue line

911
00:55:08,287 --> 00:55:12,127
And we start going downhill another one is racing it's already looking quite promising

912
00:55:12,639 --> 00:55:17,503
Now you do another restoration and you do 2000 3000 8000 9000 is Russians

913
00:55:17,759 --> 00:55:21,343
Underwear that with this green thing which perfectly goes 205 datapoint

914
00:55:21,599 --> 00:55:23,647
I have learned

915
00:55:24,159 --> 00:55:24,927
The function

916
00:55:25,439 --> 00:55:27,999
From the 58.0 maybe say

917
00:55:28,255 --> 00:55:33,887
Not rubbish that's stupid because maybe you're dealing with a noisy problem we're actually

918
00:55:34,143 --> 00:55:40,287
What you really think is going on isn't there is a true on the line function and the datapoint

919
00:55:40,543 --> 00:55:41,311
White bison

920
00:55:41,567 --> 00:55:44,895
Additive noise and then you would be rather irritated to have

921
00:55:45,151 --> 00:55:47,455
Agreeing cuz it doesn't go through every single

922
00:55:47,711 --> 00:55:49,247
Detail Livernois

923
00:55:50,271 --> 00:55:52,063
I guess you all just asked if I did not weigh

924
00:55:52,319 --> 00:55:53,343
What do you then say

925
00:55:53,599 --> 00:55:57,439
Why do you say I want to do learning with regularization

926
00:55:57,695 --> 00:55:59,999
Answer you change the objective function again

927
00:56:00,767 --> 00:56:03,839
And decide I need some sort of Regulation

928
00:56:04,351 --> 00:56:09,727
Because I don't like this really function cuz it's a bit earlier than I was expecting

929
00:56:09,983 --> 00:56:13,311
Expectation of it and not being super duper weekly

930
00:56:14,335 --> 00:56:19,199
Can be expressed with regular limes are good old regular Liza

931
00:56:19,455 --> 00:56:21,503
We had a moment ago ew

932
00:56:24,831 --> 00:56:30,975
You can add alpha x it or maybe a bit more sophisticated and you say actually I should deserve divine

933
00:56:31,231 --> 00:56:33,279
The weights in two classes because

934
00:56:33,535 --> 00:56:35,839
Remember remember to get the wiggle Inus

935
00:56:36,607 --> 00:56:37,887
Vestibule function

936
00:56:38,143 --> 00:56:38,911
It had

937
00:56:39,167 --> 00:56:43,775
A length scale it was determined by the standard deviation of infant lights

938
00:56:44,287 --> 00:56:50,431
It had a number of Wiggles that was related to the standard deviation of the bias is

939
00:56:50,943 --> 00:56:55,039
So it's actually number of Wiggles in a typical function with Cigna buying service

940
00:56:55,551 --> 00:56:56,831
Roughly

941
00:56:57,343 --> 00:57:02,719
I'm out of this cool thanks Kyle it was something to do with Sigma route to Times Square

942
00:57:02,975 --> 00:57:03,487
H

943
00:57:05,023 --> 00:57:06,303
So since the

944
00:57:07,071 --> 00:57:08,095
Input weights

945
00:57:08,351 --> 00:57:14,495
Magna shoes in the biases and outputs are affecting things in such different ways it might make sense to have three regular

946
00:57:15,519 --> 00:57:17,311
One for each of these things

947
00:57:17,823 --> 00:57:19,103
I need to Define

948
00:57:19,359 --> 00:57:23,711
Ew13 the sum of squares

949
00:57:24,735 --> 00:57:26,015
All of the weights

950
00:57:26,527 --> 00:57:29,599
Summing over the biases

951
00:57:30,111 --> 00:57:31,903
Gw2

952
00:57:32,159 --> 00:57:36,255
The Babe sum of squares with weights

953
00:57:37,791 --> 00:57:39,327
Over the input weights only

954
00:57:40,863 --> 00:57:46,495
Tw3 could be half some of WK squared

955
00:57:47,007 --> 00:57:50,335
Coming over the output lights

956
00:57:50,591 --> 00:57:56,735
Oh man that would be a more Dimension the valence thing to do and then you can say my regular Eiza is

957
00:57:56,991 --> 00:57:59,295
Some oversea goes in 1 2 3

958
00:57:59,807 --> 00:58:03,135
Alfa See Ew say and then hitting something dimensioni valid

959
00:58:03,903 --> 00:58:06,207
Robin just pretending that they all have the same

960
00:58:06,719 --> 00:58:07,999
Regularization

961
00:58:09,023 --> 00:58:12,095
So when you do that the lights on please

962
00:58:13,631 --> 00:58:19,007
Instead of the data are going ever smaller as you manage to perfectly fit the data

963
00:58:19,263 --> 00:58:23,871
And the white blowing up which is what just happened with no regularizar

964
00:58:24,127 --> 00:58:30,271
You can add on you can settle the alphas say to one just to start off with so Alpha One

965
00:58:30,527 --> 00:58:32,319
Oportun off of 301

966
00:58:32,575 --> 00:58:34,879
I know you got downhill and this whole situation looks

967
00:58:35,135 --> 00:58:38,463
Just like last time he keep on going downhill and just sit still

968
00:58:38,719 --> 00:58:40,255
I need a for 10000 its relations

969
00:58:40,511 --> 00:58:45,631
And hasn't blown up and gone terribly Wiggly to fit every detail of Illinois

970
00:58:46,655 --> 00:58:49,215
For all you might say maybe that's better

971
00:58:49,983 --> 00:58:52,031
But you might still be a bit dissatisfied because

972
00:58:52,287 --> 00:58:55,359
Now is that really the answer to a question what was the question

973
00:58:57,919 --> 00:59:02,015
Anyway the objective function that we all know minimizing cold m

974
00:59:06,111 --> 00:59:07,903
Time is equal to

975
00:59:08,415 --> 00:59:14,559
A multiple of Idi plus some of Alvin See Ew

976
00:59:18,143 --> 00:59:24,287
That's what we're not minimizing a function has going down and settle down and we found enough of it

977
00:59:24,543 --> 00:59:26,079
And the weights blown up

978
00:59:27,103 --> 00:59:33,247
Okay what's next I suspect what's next is interpretation of

979
00:59:33,503 --> 00:59:36,063
What we're just doing silent see ya okay

980
00:59:36,319 --> 00:59:37,599
Let's go back

981
00:59:37,855 --> 00:59:38,367
2

982
00:59:38,879 --> 00:59:39,903
The single neuron

983
00:59:42,975 --> 00:59:44,511
So the single neuron

984
00:59:45,791 --> 00:59:46,815
Look like that

985
00:59:47,583 --> 00:59:48,351
And I want to

986
00:59:49,119 --> 00:59:51,423
Encourage you to feel dissatisfied with this

987
00:59:51,935 --> 00:59:53,727
And feel dissatisfied with

988
00:59:55,263 --> 00:59:58,335
This thing as well that we just had a moment ago

989
00:59:58,847 --> 01:00:00,895
And then we'll fix your dissatisfaction

990
01:00:01,407 --> 01:00:07,551
So why should you feel dissatisfied with this outcome of having trained a single neuron

991
01:00:07,807 --> 01:00:10,111
As a replacement for the pigeon that discriminates

992
01:00:10,879 --> 01:00:12,159
Pebble from raisins

993
01:00:12,671 --> 01:00:13,183
Well

994
01:00:13,695 --> 01:00:14,719
Have a think

995
01:00:14,975 --> 01:00:20,095
About what

996
01:00:20,351 --> 01:00:26,495
As you set altitude a different value you got a different answer for 2.1 different solution

997
01:00:26,751 --> 01:00:32,895
And he said off of one and you got two different solution against one issue is how do you set the timer

998
01:00:34,175 --> 01:00:39,039
But another issue even if you were happy with the with some magic that was used to set it to this value

999
01:00:39,295 --> 01:00:40,319
You might say

1000
01:00:42,623 --> 01:00:48,767
I'm willing to believe that the predictions hereabouts are quite well tolerated to this day so I think

1001
01:00:49,023 --> 01:00:53,375
Look for you both. Yeah yeah so 20% is shared 80%

1002
01:00:53,631 --> 01:00:56,447
But I don't believe this extrapolation over here

1003
01:00:56,959 --> 01:00:57,471
So

1004
01:00:57,727 --> 01:00:59,519
If you look at points

1005
01:01:00,799 --> 01:01:02,591
Cold A&B

1006
01:01:03,871 --> 01:01:05,663
Are you happy with the idea

1007
01:01:05,919 --> 01:01:08,735
That would just as confident a is soda

1008
01:01:09,247 --> 01:01:11,807
90% certain to be in the other class

1009
01:01:12,063 --> 01:01:15,391
And bees overnight intensive to be in the other class

1010
01:01:15,647 --> 01:01:16,671
Is that reasonable

1011
01:01:17,695 --> 01:01:20,767
If you are dissatisfied you might be interested in

1012
01:01:21,535 --> 01:01:24,095
The view of learning as

1013
01:01:24,351 --> 01:01:25,119
Inference

1014
01:01:26,911 --> 01:01:27,679
So we discussed

1015
01:01:27,935 --> 01:01:30,239
Learning as communication

1016
01:01:30,751 --> 01:01:34,079
And I was going to discuss learning with legalization and weight gain

1017
01:01:35,871 --> 01:01:36,895
Inference

1018
01:01:59,935 --> 01:02:02,495
So if someone minimizes something

1019
01:02:02,751 --> 01:02:04,799
AR-10 today fine

1020
01:02:05,567 --> 01:02:11,199
I will exponentially the thing you'll minimizing and I'm going to interpret as a probability whether you like it or not

1021
01:02:11,455 --> 01:02:17,087
So someone take mmw which is DFW plus Alpha ew

1022
01:02:17,343 --> 01:02:19,135
I minimize this

1023
01:02:19,391 --> 01:02:20,415
I will say

1024
01:02:20,927 --> 01:02:21,695
All right

1025
01:02:23,231 --> 01:02:26,047
I'm going to interpret that in the following way

1026
01:02:26,559 --> 01:02:30,399
The posterior probability of the weights given the implicit assumptions you're making

1027
01:02:31,167 --> 01:02:34,239
Is Easter - mfw

1028
01:02:34,495 --> 01:02:36,031
On that

1029
01:02:36,799 --> 01:02:39,103
And that is equal to the product of

1030
01:02:39,359 --> 01:02:41,663
The probability of the data

1031
01:02:41,919 --> 01:02:43,455
Giving w

1032
01:02:43,967 --> 01:02:47,039
And the probability of w

1033
01:02:47,295 --> 01:02:49,343
Giving all your assumptions

1034
01:02:50,367 --> 01:02:56,511
So this is your poster distribution given the data on your other assumptions and that needs normalizing with something

1035
01:03:00,095 --> 01:03:05,471
Implicitly if you minimize this thing you are finding the most probable W given these assumptions

1036
01:03:05,727 --> 01:03:09,823
The probability of the data given W is e to the G

1037
01:03:10,335 --> 01:03:11,359
Off

1038
01:03:11,871 --> 01:03:12,895
W

1039
01:03:14,431 --> 01:03:19,551
Find the probability of w the probability is 82 - Alpha

1040
01:03:19,807 --> 01:03:20,831
UW

1041
01:03:21,343 --> 01:03:22,623
On something

1042
01:03:28,767 --> 01:03:34,911
So this is an interpretation of what people are already doing it doesn't change anything but it may add some useful

1043
01:03:35,167 --> 01:03:35,679
Perspectives

1044
01:03:36,191 --> 01:03:38,495
Sorry if someone minimizes this objective function

1045
01:03:38,751 --> 01:03:40,543
They are implicitly assuming that

1046
01:03:41,055 --> 01:03:42,335
It really is the case

1047
01:03:42,591 --> 01:03:48,479
That does an underlying function y that generated the day to buy flipping been coins with a bias of why

1048
01:03:48,991 --> 01:03:51,295
So that's the interpretation of

1049
01:03:51,551 --> 01:03:52,831
Data

1050
01:03:53,599 --> 01:03:54,111
Came

1051
01:03:54,367 --> 01:03:55,391
From

1052
01:03:55,647 --> 01:03:57,695
Why using a bent coin

1053
01:03:58,719 --> 01:03:59,999
Golf bios

1054
01:04:00,511 --> 01:04:01,535
Why

1055
01:04:01,791 --> 01:04:02,815
That's what this means

1056
01:04:03,583 --> 01:04:04,351
End

1057
01:04:05,375 --> 01:04:10,239
The regular eyes of it being used to penalizing on with weights because we decided we didn't like it always late

1058
01:04:10,751 --> 01:04:15,871
Is interpreted as a private Leaf about the weights that says if ew is

1059
01:04:16,127 --> 01:04:22,271
Pop W Squared example it's saying that the prior probability of w is a normal distribution

1060
01:04:23,807 --> 01:04:25,087
With mean 0

1061
01:04:25,855 --> 01:04:29,183
End with standard deviation Sigma squared

1062
01:04:29,695 --> 01:04:30,975
W

1063
01:04:31,231 --> 01:04:36,095
That is equal to 1 / alpha alpha

1064
01:04:36,351 --> 01:04:39,679
I'm going to go with a pain in the neck Alba tree regularization compliment

1065
01:04:39,935 --> 01:04:41,983
Is now interpreted as being the variance

1066
01:04:42,239 --> 01:04:43,519
Have a prior distribution

1067
01:04:47,615 --> 01:04:49,151
Misinterpretation can be

1068
01:04:49,663 --> 01:04:52,223
Ignored or it can be exploited

1069
01:04:54,271 --> 01:04:59,135
The exportation says okay let's take this literally and then we can do a whole load of new things

1070
01:04:59,391 --> 01:05:01,183
For example we can actually ask

1071
01:05:02,463 --> 01:05:05,535
What's the right way to turn the entrance into a prediction

1072
01:05:06,559 --> 01:05:09,119
If someone asks for a prediction what should you do

1073
01:05:09,631 --> 01:05:11,679
Well if you believe is most correct

1074
01:05:12,191 --> 01:05:18,335
And you're asked to predict what's the probability that the next thing that we just observe at Lowe's

1075
01:05:18,591 --> 01:05:21,407
Location x + x + + Wong

1076
01:05:21,919 --> 01:05:28,063
Is in class 1 what's the probability this new point on the screen

1077
01:05:28,319 --> 01:05:31,135
What's the probability that it's a potato

1078
01:05:31,391 --> 01:05:33,439
4 times table

1079
01:05:34,207 --> 01:05:39,583
Given the data we've seen so far and all the other assumptions

1080
01:05:40,863 --> 01:05:43,679
Well the answer to that question isn't found by saying

1081
01:05:43,935 --> 01:05:50,079
Please welcome the W that minimizes this and then use that to make predictions answers done

1082
01:05:50,335 --> 01:05:56,479
So the correct answer if you believe Ali Simpsons is you should marginalised / W + 2

1083
01:05:56,735 --> 01:05:58,527
BMW

1084
01:05:58,783 --> 01:06:02,111
* the prediction which is why

1085
01:06:02,367 --> 01:06:05,183
Evaluated at x n + 1

1086
01:06:05,439 --> 01:06:07,487
Using the premises

1087
01:06:07,743 --> 01:06:08,255
W

1088
01:06:08,511 --> 01:06:12,863
So that's what you actually ought to do if you believe the assumptions that you've been personally

1089
01:06:13,119 --> 01:06:13,887
Be making

1090
01:06:15,679 --> 01:06:17,215
That's an example of

1091
01:06:17,727 --> 01:06:19,775
An average value of a simple function

1092
01:06:20,031 --> 01:06:21,567
Under a nasty red

1093
01:06:21,823 --> 01:06:22,591
Distribution

1094
01:06:23,871 --> 01:06:25,663
And we know three ways

1095
01:06:26,431 --> 01:06:27,455
Elephant

1096
01:06:27,967 --> 01:06:29,503
We can use Monte Carlo methods

1097
01:06:30,015 --> 01:06:31,807
We can use variational method

1098
01:06:33,599 --> 01:06:34,111
Or

1099
01:06:34,879 --> 01:06:36,159
We can use

1100
01:06:37,695 --> 01:06:38,719
The prices method

1101
01:06:41,279 --> 01:06:42,303
So let's do that

1102
01:06:43,583 --> 01:06:49,727
So just to make clear what's red then nasty red distribution is this thing the posterior probability of the weight

1103
01:06:49,983 --> 01:06:50,751
Thanksgiving the date

1104
01:06:51,007 --> 01:06:51,519
I'm not

1105
01:06:52,031 --> 01:06:53,055
Is this thing here

1106
01:06:54,591 --> 01:06:55,871
It's a nasty red thing

1107
01:06:56,127 --> 01:07:01,247
But we can compute house before Me 2 - time of w and M we have powder formula for

1108
01:07:02,783 --> 01:07:04,831
So we can attack this in a variety of ways

1109
01:07:05,599 --> 01:07:09,695
Sumfest way I will use is cooled multicolor

1110
01:07:10,463 --> 01:07:13,535
So we can take Handbook of Monte Carlo methods

1111
01:07:14,303 --> 01:07:14,815
And

1112
01:07:15,071 --> 01:07:16,095
We can say

1113
01:07:16,607 --> 01:07:17,375
Oh goodness me

1114
01:07:18,655 --> 01:07:21,471
We can say 45 slides

1115
01:07:25,823 --> 01:07:26,591
Right

1116
01:07:26,847 --> 01:07:27,871
We can say

1117
01:07:28,127 --> 01:07:30,175
Let's give us at the Monte Carlo method

1118
01:07:33,247 --> 01:07:34,527
Used Monte Carlo

1119
01:07:35,807 --> 01:07:38,111
To evaluate this quantity k

1120
01:07:39,135 --> 01:07:40,159
How different is that

1121
01:07:40,671 --> 01:07:41,439
Well if we use

1122
01:07:41,951 --> 01:07:48,095
Hamiltonian Monte Carlo hamiltonian if you like to the guy that involves Computing the gradient and then going

1123
01:07:48,351 --> 01:07:51,679
Downhill with momentum and randomizing momentum from time to time

1124
01:07:52,447 --> 01:07:56,799
A very simple version of the hamiltonian want to call him at the Disco belongs remind message

1125
01:07:57,055 --> 01:07:58,079
Which involves

1126
01:07:59,615 --> 01:08:01,919
Going downhill a little bit and adding some noise

1127
01:08:03,711 --> 01:08:06,783
So how does that different different from steepest to sense

1128
01:08:07,551 --> 01:08:09,599
The difference by adding a better noise

1129
01:08:10,111 --> 01:08:11,647
San Francisco's on Hill

1130
01:08:11,903 --> 01:08:14,975
What we're going to do now is do seems to sense with a bit of noise

1131
01:08:15,999 --> 01:08:19,327
I'd rather than accept reject decision in there as well

1132
01:08:20,351 --> 01:08:22,399
So we can use the lawn Divine method

1133
01:08:22,655 --> 01:08:25,471
And if we go back to this thing

1134
01:08:28,543 --> 01:08:29,823
What will now see

1135
01:08:30,079 --> 01:08:30,847
Is

1136
01:08:34,175 --> 01:08:36,479
The laundromat head stomping from

1137
01:08:36,735 --> 01:08:38,271
This place of the origin

1138
01:08:38,527 --> 01:08:42,367
Where we used to just go downhill and his way we ended up having going downhill

1139
01:08:42,623 --> 01:08:48,767
What are you just saying that have computer time whole we have to do is go downhill and I have some nice there's nothing nothing huge costs

1140
01:08:49,023 --> 01:08:55,167
Fancy Bayesian inference method you just add some noise at the right amount of noise putting put in

1141
01:08:55,423 --> 01:08:56,959
September Jax to make sure it's old

1142
01:08:57,215 --> 01:08:58,239
Correct

1143
01:08:59,775 --> 01:09:02,079
His what happens I'm going to zoom in on the 1st

1144
01:09:02,335 --> 01:09:04,127
Elbow of the optimization

1145
01:09:04,639 --> 01:09:05,919
So that's the first album

1146
01:09:06,175 --> 01:09:08,223
Enjoying the same amount of computer time

1147
01:09:08,479 --> 01:09:09,759
His way we got to

1148
01:09:11,039 --> 01:09:16,415
With laundromat episode

1149
01:09:17,695 --> 01:09:18,975
Victoria Lee

1150
01:09:19,231 --> 01:09:21,023
What you can think of happening

1151
01:09:21,279 --> 01:09:22,559
If we used to go

1152
01:09:22,815 --> 01:09:26,911
NW spay still be wrong W-2 and it's actually three dimensional for this

1153
01:09:27,167 --> 01:09:29,727
Play problem we used to go downhill to an ultimate

1154
01:09:29,983 --> 01:09:35,871
Now we're feeling that as the minimum of a read function which has some

1155
01:09:36,127 --> 01:09:37,919
Strange shaped like this

1156
01:09:38,943 --> 01:09:43,807
And we're going downhill and adding noise in such a way as the sample from that entire red

1157
01:09:44,063 --> 01:09:44,831
Distribution

1158
01:09:46,367 --> 01:09:49,439
I got his what happens after 40000 its relations

1159
01:09:49,695 --> 01:09:51,743
We're going to leave samples

1160
01:09:52,255 --> 01:09:53,535
From the entire red

1161
01:09:53,791 --> 01:09:54,815
Distribution

1162
01:09:55,839 --> 01:09:58,911
And we can take those sample points

1163
01:09:59,423 --> 01:10:00,447
And add up

1164
01:10:01,983 --> 01:10:05,567
The predictions why for each of those so we can approximate

1165
01:10:07,615 --> 01:10:10,175
That's using the sound of Monte Carlo idea

1166
01:10:10,687 --> 01:10:11,711
Bye

1167
01:10:11,967 --> 01:10:14,271
This is approximately one

1168
01:10:14,527 --> 01:10:16,063
Some

1169
01:10:16,319 --> 01:10:18,111
Or is one

1170
01:10:18,623 --> 01:10:21,695
Y X + + 1

1171
01:10:23,487 --> 01:10:24,511
Using weights

1172
01:10:24,767 --> 01:10:28,607
Weather Eastlake wi come from

1173
01:10:30,911 --> 01:10:31,679
Through

1174
01:10:32,703 --> 01:10:33,983
The miracle of Monte Carlo

1175
01:10:34,239 --> 01:10:35,775
True hamiltonian

1176
01:10:36,031 --> 01:10:37,055
Monte Carlo

1177
01:10:37,823 --> 01:10:40,639
Write up the plants or just need to sum up all of them

1178
01:10:41,151 --> 01:10:42,687
And I'll do that right now

1179
01:10:44,223 --> 01:10:46,783
So I'm not going to use old 40,000 I'll just pick

1180
01:10:48,831 --> 01:10:50,367
$100 ice samples

1181
01:10:50,623 --> 01:10:56,767
His what the weights were doing with time wandering up and down in an older car related way that's what they would

1182
01:10:57,023 --> 01:10:59,583
Would have done if we just optimize I would have settled down and

1183
01:10:59,839 --> 01:11:04,447
They still thought I would doing something more interesting bubbling around looking at Poolesville outcomes

1184
01:11:04,959 --> 01:11:05,727
And this is

1185
01:11:06,239 --> 01:11:11,615
What it looks like if we take those hundred samples and I will. Visualize some of them for you in

1186
01:11:12,127 --> 01:11:15,199
Infospace I'm going to show you 30 of those functions

1187
01:11:15,455 --> 01:11:16,991
30 of these functions

1188
01:11:17,247 --> 01:11:18,271
Why

1189
01:11:19,039 --> 01:11:19,807
Xn

1190
01:11:20,319 --> 01:11:23,391
For the different values of w

1191
01:11:23,903 --> 01:11:25,695
I'm going to show you the ex dependents now

1192
01:11:26,463 --> 01:11:26,975
All right

1193
01:11:27,231 --> 01:11:33,375
And the concept is these a 30 typical plausible ideas about how to explain today's hits

1194
01:11:34,399 --> 01:11:35,423
3

1195
01:11:35,679 --> 01:11:36,191
4

1196
01:11:36,703 --> 01:11:37,215
5

1197
01:11:37,727 --> 01:11:38,239
6

1198
01:11:38,495 --> 01:11:43,103
7 through these noticed that the classification of b sometimes changes

1199
01:11:43,359 --> 01:11:44,895
Look it's on the blue side now

1200
01:11:46,431 --> 01:11:48,223
Still on the blue side now it's on the other side

1201
01:11:48,479 --> 01:11:50,015
And the conservation of a

1202
01:11:50,271 --> 01:11:52,831
Doesn't change the match is on the other side yellow

1203
01:11:53,087 --> 01:11:57,183
Just a little bit of blue yellow yellow yellow yellow

1204
01:11:57,439 --> 01:12:00,767
So when we add all of these together you can anticipate

1205
01:12:01,279 --> 01:12:04,607
So the answer for a is not going to be the same if you could be any more

1206
01:12:05,119 --> 01:12:05,631
Verizon

1207
01:12:05,887 --> 01:12:07,935
When we approximated this

1208
01:12:08,447 --> 01:12:10,751
Integral hair in a really stupid way

1209
01:12:11,007 --> 01:12:12,031
By saying

1210
01:12:13,311 --> 01:12:14,847
Let's approximate this

1211
01:12:15,359 --> 01:12:16,127
As

1212
01:12:17,663 --> 01:12:19,455
Bendigo VW

1213
01:12:20,479 --> 01:12:22,783
Delta function at w

1214
01:12:23,039 --> 01:12:24,319
Set to W Starr

1215
01:12:25,087 --> 01:12:28,927
X Y of x w

1216
01:12:29,951 --> 01:12:34,815
That's a really stupid approximation that says let's replace the whole distribution by had a spike

1217
01:12:37,631 --> 01:12:39,935
When we did that. The same answers be

1218
01:12:40,447 --> 01:12:41,983
Tonight let's do it right

1219
01:12:42,495 --> 01:12:44,031
How many do it right

1220
01:12:44,287 --> 01:12:46,335
Given the assumptions we just nice

1221
01:12:46,591 --> 01:12:49,663
You got not this which is the answer with the ultimate promises

1222
01:12:49,919 --> 01:12:50,431
But

1223
01:12:52,479 --> 01:12:53,247
And you might say

1224
01:12:53,503 --> 01:12:56,063
I prefer that in which case you might like

1225
01:12:56,319 --> 01:12:57,599
Bayesian learning and inference

1226
01:12:59,135 --> 01:13:01,695
Okay so that was one method

1227
01:13:02,463 --> 01:13:04,255
Another method is to say

1228
01:13:05,791 --> 01:13:10,143
I don't want to do Monte Carlo I want to use something a bit more deterministic

1229
01:13:10,655 --> 01:13:11,935
So you could say

1230
01:13:12,447 --> 01:13:14,495
I'm going to approximate this

1231
01:13:14,751 --> 01:13:15,775
Fly

1232
01:13:16,031 --> 01:13:18,079
Indigo DW

1233
01:13:18,847 --> 01:13:21,151
A nice distribution Q

1234
01:13:23,199 --> 01:13:24,991
Have a w given data

1235
01:13:25,503 --> 01:13:27,039
Or are they giving Pizza

1236
01:13:29,599 --> 01:13:30,367
X y

1237
01:13:31,647 --> 01:13:33,439
So you could say

1238
01:13:34,719 --> 01:13:37,535
I can't cope with this Monte Carlo integration approach

1239
01:13:37,791 --> 01:13:41,375
But if you had some simple form for example

1240
01:13:41,631 --> 01:13:42,911
A normal distribution

1241
01:13:44,959 --> 01:13:49,311
Then maybe I could come compute this fairly quickly and simply

1242
01:13:50,335 --> 01:13:51,103
So

1243
01:13:51,359 --> 01:13:54,175
The approach that I'll show you now says

1244
01:13:54,431 --> 01:13:56,223
Let's use the prices method

1245
01:13:59,295 --> 01:14:05,439
To come up with a normal approximation and it will fit into that formula and do the last step

1246
01:14:05,695 --> 01:14:07,231
Numerically which can be done

1247
01:14:07,487 --> 01:14:08,255
Are you silly

1248
01:14:09,023 --> 01:14:10,815
So Lights On Again

1249
01:14:13,375 --> 01:14:17,215
Just show you these a few more times could you get a nice soda

1250
01:14:17,727 --> 01:14:19,519
Illusion of non-parallel with

1251
01:14:20,799 --> 01:14:26,943
Okay this is a few more laundry bomb pictures before we we do LaPlace

1252
01:14:27,199 --> 01:14:30,527
Contrast of the objective function that went down when we minimized it

1253
01:14:30,783 --> 01:14:35,903
The laundromat that goes down a little over the place because we're not asking the optimizers what devices anymore

1254
01:14:36,159 --> 01:14:42,303
We didn't actually want to optimize it because we are not interested in the best primer interested in

1255
01:14:42,559 --> 01:14:44,607
What Floresville settings of the premises are

1256
01:14:45,119 --> 01:14:45,887
Okay

1257
01:14:48,959 --> 01:14:51,519
So what's next laplace's method

1258
01:14:52,031 --> 01:14:54,079
Here is the gaussian approximation

1259
01:14:54,591 --> 01:15:00,735
It is as a function of W1 W2 and W3 which you can see the three weights

1260
01:15:02,271 --> 01:15:03,039
And

1261
01:15:03,295 --> 01:15:06,623
It's Augustine distribution that I projected down onto

1262
01:15:06,879 --> 01:15:08,159
This W on W-2

1263
01:15:08,415 --> 01:15:10,463
Space for this particular figure hair

1264
01:15:10,975 --> 01:15:12,511
And when you

1265
01:15:13,023 --> 01:15:17,887
Integrate. The weights and then compute the numerical integral that you left with

1266
01:15:19,167 --> 01:15:25,311
You can notice that the guys going to mention isn't necessarily very good one and send play Kiss that the yellow samples from the posterior with the law

1267
01:15:25,567 --> 01:15:26,079
Songs about method

1268
01:15:26,591 --> 01:15:30,943
And that doesn't look at all like samples from the green gas you so it's not a very good approximation

1269
01:15:31,199 --> 01:15:34,271
But nevertheless is better than not bothering at all

1270
01:15:34,527 --> 01:15:37,343
And it was the predictions reopen micrometers

1271
01:15:37,855 --> 01:15:42,463
His predictions we got from the launch of my method and his what you got from the gas station

1272
01:15:42,719 --> 01:15:45,535
APA citing for a banana

1273
01:15:45,791 --> 01:15:47,327
Numerical detail

1274
01:15:47,583 --> 01:15:48,607
It's

1275
01:15:48,863 --> 01:15:49,631
A big difference

1276
01:15:50,143 --> 01:15:51,423
Okay

1277
01:15:51,679 --> 01:15:53,215
I think that's the end of

1278
01:15:53,471 --> 01:15:53,983
The

1279
01:15:54,239 --> 01:15:56,543
Learning is inference for the single neuron

1280
01:15:57,823 --> 01:16:00,127
We cannot go through the same process

1281
01:16:00,383 --> 01:16:01,151
4

1282
01:16:02,175 --> 01:16:02,943
RV

1283
01:16:03,455 --> 01:16:06,783
Feed-forward Network as well so let's go back

1284
01:16:15,999 --> 01:16:16,511
So

1285
01:16:16,767 --> 01:16:17,791
I'm going to go

1286
01:16:18,303 --> 01:16:24,447
We were I hope dissatisfied because the Greenline went perfectly through all the right points which is what we had to ask it to do

1287
01:16:24,703 --> 01:16:30,847
And I was going to say we don't like that let's use the lungs by method to sample from the posterior exact interpretation

1288
01:16:31,103 --> 01:16:32,383
The two times the date the time

1289
01:16:32,639 --> 01:16:33,663
And the wait times

1290
01:16:33,919 --> 01:16:36,735
Are the likelihood unlock sale in the prior

1291
01:16:36,991 --> 01:16:39,039
Of a Bayesian distribution

1292
01:16:39,551 --> 01:16:40,063
And

1293
01:16:40,575 --> 01:16:46,719
We use the lungs by method in just the same way so instead of competing the gradient and going downhill you compute the grade in

1294
01:16:46,975 --> 01:16:48,767
Go downhill and add some nice

1295
01:16:50,303 --> 01:16:53,887
That means that the first few steps look just the same as before

1296
01:16:54,143 --> 01:17:00,287
But now you got Billy Billy Billy Billy Billy instead of just settling down and going to a boarding Optimum

1297
01:17:00,799 --> 01:17:03,615
And you can pick from those

1298
01:17:03,871 --> 01:17:10,015
30000 it's rationed a few representative samples for example a dozen is probably enough so many problems

1299
01:17:10,271 --> 01:17:13,087
And those doesn't samples can be used to predict things like

1300
01:17:13,343 --> 01:17:19,487
What's your mean what's your variance and so forth so I can extract from these 12 samples which show 12

1301
01:17:19,743 --> 01:17:22,559
Credible hypotheses of the underlying function is

1302
01:17:22,815 --> 01:17:26,399
Assuming that it was made by a sum of hydraulic engines

1303
01:17:26,911 --> 01:17:29,471
That's 12 hypotheses and

1304
01:17:30,239 --> 01:17:31,263
Hair is the mean

1305
01:17:31,519 --> 01:17:32,031
And

1306
01:17:32,287 --> 01:17:33,311
Standard deviation

1307
01:17:33,567 --> 01:17:35,359
That you got from those 12 samples

1308
01:17:35,615 --> 01:17:41,759
Notice the standard deviation small wave data

1309
01:17:42,015 --> 01:17:43,295
Lighting and it's ultra-wide onu

1310
01:17:43,551 --> 01:17:45,087
Interpolating of 1/2

1311
01:17:45,599 --> 01:17:51,487
So that's against showing the benefit of using a Bayesian

1312
01:17:51,999 --> 01:17:53,791
View of this little learning

1313
01:17:54,047 --> 01:18:00,191
Protest and there are many other benefits that I won't go into now for example you can get automatic complexity control the volume

1314
01:18:00,447 --> 01:18:04,543
Alpha 1 Alpha 2 Alpha 3 become increasingly important

1315
01:18:04,799 --> 01:18:07,103
The mold I mentioned you you're working in

1316
01:18:07,615 --> 01:18:13,759
And you can use Bayesian methods to automatically info what should I have one after another for 3B

1317
01:18:14,015 --> 01:18:17,855
Given the date of it I've got so you can get rid of that headache as well

1318
01:18:18,111 --> 01:18:19,391
With the Bayesian approach

1319
01:18:20,415 --> 01:18:25,791
Right is there anymore in that his a graph showing what happened to the objective function

1320
01:18:26,047 --> 01:18:27,327
And that

1321
01:18:27,583 --> 01:18:28,607
Is that

1322
01:18:29,119 --> 01:18:31,167
Okay so I'm showing you the

1323
01:18:32,191 --> 01:18:34,751
Interpretation of learning as inference

1324
01:18:35,007 --> 01:18:41,151
And I want to wrap up my app I just telling you a few applications I'll be so to feed forward neural network methods

1325
01:18:41,407 --> 01:18:44,479
So these are all quite old now because I wrote this lines

1326
01:18:44,735 --> 01:18:47,295
A while ago and they were some

1327
01:18:47,551 --> 01:18:49,599
Interesting examples at the time

1328
01:18:49,855 --> 01:18:53,183
On the remote have come in in the intervening years

1329
01:18:54,719 --> 01:18:55,231
The

1330
01:18:55,999 --> 01:19:02,143
Very earliest application that I came across of using feed-forward neural networks to solve interest

1331
01:19:02,399 --> 01:19:03,935
Problems with the task of

1332
01:19:04,447 --> 01:19:05,215
Getting it

1333
01:19:05,471 --> 01:19:09,567
A neural network to read aloud and quotes the way that what was

1334
01:19:10,079 --> 01:19:11,103
The input

1335
01:19:11,359 --> 01:19:14,687
Represented seven successive Leches in a piece of text

1336
01:19:15,199 --> 01:19:17,247
The hidden units. To see old 7 lashes

1337
01:19:17,503 --> 01:19:19,551
Do some stuff and spit out

1338
01:19:19,807 --> 01:19:25,951
Output which is not just one number but rather a multi-dimensional representation of a phoneme so there was a binary

1339
01:19:26,207 --> 01:19:29,023
Encoding of the input in a binary encoding of phonemes

1340
01:19:29,279 --> 01:19:30,303
And the

1341
01:19:30,559 --> 01:19:36,447
Training Albert insalaco text and correct pronunciations and see if we can learn in general

1342
01:19:36,959 --> 01:19:41,311
That was an early success of neural networks cuz it didn't work

1343
01:19:41,567 --> 01:19:42,079
Too badly

1344
01:19:42,847 --> 01:19:48,991
Another very significant success which I think still has not really being sick

1345
01:19:49,247 --> 01:19:50,015
Beachin

1346
01:19:50,527 --> 01:19:52,063
Is the use of mokolea

1347
01:19:52,319 --> 01:19:55,903
Free photo that works to do genuine handwriting recognition

1348
01:19:56,159 --> 01:19:59,999
This was done by Young laocoon and colleagues at AT&T

1349
01:20:01,279 --> 01:20:03,327
And his an example of

1350
01:20:03,583 --> 01:20:09,727
An input to the network which is a three and a four with some scribbles on it and it's called zillions of hitting you

1351
01:20:09,983 --> 01:20:13,824
Units on the nap at the top and it says this is Rena for

1352
01:20:14,336 --> 01:20:19,456
And it's very impressive thing I was mean to getting my browser web page showing you

1353
01:20:19,968 --> 01:20:25,088
A demonstration of this thing you can you can hunt for it on for Lynette on the international

1354
01:20:25,344 --> 01:20:26,112
Hussy

1355
01:20:26,368 --> 01:20:29,440
The range of things that it could correctly

1356
01:20:29,696 --> 01:20:30,208
Classify

1357
01:20:30,976 --> 01:20:35,840
Another example that I was involved in was the muddling of weld toughness

1358
01:20:37,120 --> 01:20:38,656
Weld toughness is

1359
01:20:38,912 --> 01:20:40,192
A function of

1360
01:20:40,704 --> 01:20:43,776
Cookery essentially you cook the well by putting in various

1361
01:20:44,032 --> 01:20:50,176
Impurities and picking and eating temperatures and so forth in the weld with a whole range of properties

1362
01:20:50,432 --> 01:20:51,712
Toughness as a strength

1363
01:20:51,968 --> 01:20:54,272
Yield yield strength and

1364
01:20:54,528 --> 01:20:57,088
Ultimate tensile strength answer for and

1365
01:20:57,344 --> 01:21:03,488
If you're going to use a welding a particular component for example these things on the floor Power Station components and I need to be well

1366
01:21:05,024 --> 01:21:07,840
You want the well to have correct

1367
01:21:08,096 --> 01:21:08,864
Properties

1368
01:21:09,120 --> 01:21:11,936
And there are many ways of trying to model weld

1369
01:21:12,192 --> 01:21:13,728
Properties like toughness

1370
01:21:13,984 --> 01:21:16,544
But if it's a really complicated thing and toughness is

1371
01:21:16,800 --> 01:21:21,920
Maybe it's a good idea just to get data and train a neural network on it and that's what everybody Sharon die

1372
01:21:22,432 --> 01:21:23,712
Did and

1373
01:21:24,736 --> 01:21:27,040
This is a real photo

1374
01:21:28,064 --> 01:21:33,952
Boxes of welding stuff this is a photo of the real thing that was made with the help of a neural network

1375
01:21:34,208 --> 01:21:40,096
These components are about to be welded together with the stuff that was designed using the neural network in his the man who killed his

1376
01:21:40,352 --> 01:21:43,168
To do welding he's just having a practice before we have to Weld stick

1377
01:21:43,424 --> 01:21:44,192
The real components

1378
01:21:44,448 --> 01:21:47,264
Services used for real at Siemens

1379
01:21:47,520 --> 01:21:53,664
And it's estimated that the neural-net approach save significant resources of

1380
01:21:54,176 --> 01:22:00,320
Compared to the alternative which would be to make a weld test it and then make another welding test it then people changing big the cook

1381
01:22:00,576 --> 01:22:01,856
Capri by by hand

1382
01:22:02,624 --> 01:22:08,768
So that's a success and we probably two dozen papers on

1383
01:22:09,024 --> 01:22:13,120
Modeling various properties of wells and other types of metal

1384
01:22:15,168 --> 01:22:21,312
Another example that like makes like the back of time was a nature paper by Rodger Angell which describes how you cook

1385
01:22:21,568 --> 01:22:24,640
Could focus multiple mirror telescopes using neural network

1386
01:22:24,896 --> 01:22:31,040
The input to the neural network was the slightly fuzzy slightly out-of-focus image coming from

1387
01:22:31,296 --> 01:22:33,088
The multiple mirror telescope

1388
01:22:33,344 --> 01:22:33,856
Indeed

1389
01:22:34,112 --> 01:22:40,256
Output is the action that you should apply right now to go telescope to to clean up the the the fuzziness in

1390
01:22:40,512 --> 01:22:42,816
Non Focus plus out of the North Pole

1391
01:22:43,072 --> 01:22:44,096
Mirror images

1392
01:22:44,352 --> 01:22:49,984
Very up imposed on proton tricky problem solved by other methods but neural networks

1393
01:22:50,240 --> 01:22:52,032
We're able to do a grand job

1394
01:22:53,312 --> 01:22:57,664
So we talked through the probabilistic interpretation of learning and

1395
01:22:57,920 --> 01:23:02,784
You might say oh so will these neural networks it's just High dimensional cuz fishing isn't it

1396
01:23:03,040 --> 01:23:07,648
And that is exactly correct for the ones I just been telling you about these multi-level set from

1397
01:23:07,904 --> 01:23:09,440
Networks

1398
01:23:09,952 --> 01:23:11,232
Coffee Food Network

1399
01:23:11,488 --> 01:23:17,632
Really all just doing high-dimensional curve fitting if it's expedient to use a neural network fine but if you have other meth

1400
01:23:17,888 --> 01:23:24,032
Methods of doing high-dimensional coefficient that you are content with and don't get on your nerves and you could use those

1401
01:23:24,288 --> 01:23:30,432
As well as a concrete example of another way of doing high-dimensional coefficient is gas in processes which I described

1402
01:23:30,944 --> 01:23:36,064
In the book so you don't have to use neural networks you could just use a gas in process which is a high dimensional

1403
01:23:36,320 --> 01:23:37,344
Company Mobile

1404
01:23:39,392 --> 01:23:45,536
Does the brain only do high damage will cut 15 well I suspect the answer is no we don't know how brains work but I do think

1405
01:23:45,792 --> 01:23:51,936
The brains are more exciting things in your brain can do I don't know if you seen this picture before it's a picture

1406
01:23:52,192 --> 01:23:53,472
Of an animal

1407
01:23:54,496 --> 01:23:55,264
And

1408
01:23:55,520 --> 01:23:58,336
Can you see what it is hands up if you couldn't recognize it

1409
01:23:58,592 --> 01:23:59,360
Okay

1410
01:23:59,616 --> 01:24:02,176
Sorry it's a dog and it's facing that way

1411
01:24:02,432 --> 01:24:04,480
And it's so facing away from us

1412
01:24:05,760 --> 01:24:06,528
The dalmatian

1413
01:24:07,552 --> 01:24:08,320
And

1414
01:24:08,832 --> 01:24:09,856
Baptist are

1415
01:24:10,624 --> 01:24:11,904
Never say it not hands up

1416
01:24:14,208 --> 01:24:14,720
So

1417
01:24:16,256 --> 01:24:22,400
I find it hard to imagine that this is just a feed-forward multi-dimensional company exercise and when I look at

1418
01:24:22,656 --> 01:24:28,800
Image like this I find it so exciting and it feels like my brain is coming out with hypotheses and isn't active

1419
01:24:29,056 --> 01:24:29,568
Participant

1420
01:24:29,824 --> 01:24:35,968
Maybe coming up an explanation for the wells and sexual

1421
01:24:36,224 --> 01:24:40,832
And those anatomical evidence that they aren't just free food that works as well

1422
01:24:41,088 --> 01:24:47,232
So in the next like that will come back to the meet one of the most exciting questions which is how can we make contact address book

1423
01:24:47,488 --> 01:24:50,048
Memories and describe a very simple neural network

1424
01:24:50,304 --> 01:24:51,328
The console

1425
01:24:51,584 --> 01:24:54,400
I counted restful memory challenge here's the challenge

1426
01:24:54,656 --> 01:25:00,800
The challenges to make a dynamical system it should be a system with 25 degrees of freedom 25 variables that

1427
01:25:01,056 --> 01:25:02,080
Change with time

1428
01:25:03,104 --> 01:25:06,432
The dynamical system can have 300 tunable parameters

1429
01:25:06,944 --> 01:25:09,760
And those promises have full bits of precision

1430
01:25:10,784 --> 01:25:12,064
The challenge is

1431
01:25:12,576 --> 01:25:17,952
You are going to be given some designs memories and those memories should be fixed points of the Dynamics

1432
01:25:18,464 --> 01:25:23,840
So the memories might look like this here are 325 dimensional patents they look like a d a j in the sea

1433
01:25:24,608 --> 01:25:30,752
And you got to come up with a way of instantly saying okay I will set the 300 parameters in the following way and I will

1434
01:25:31,008 --> 01:25:32,288
Bose

1435
01:25:32,544 --> 01:25:33,312
So that

1436
01:25:33,568 --> 01:25:34,080
If you

1437
01:25:34,336 --> 01:25:40,480
Set off from a noisy version of one of those desired memories like this which is actually annoyed CD design

1438
01:25:40,736 --> 01:25:44,576
Dynamics system should take you to this which is a clean-up day

1439
01:25:45,088 --> 01:25:48,160
If I give you this as soon as you see you should end up with this

1440
01:25:48,416 --> 01:25:50,976
And if I give you this annoy CJ

1441
01:25:51,232 --> 01:25:52,256
You should end up with this

1442
01:25:52,768 --> 01:25:58,912
That's the challenge to come up with a way of making a dynamical system and setting its parameters so that for any choice

1443
01:25:59,168 --> 01:26:01,472
Or almost any choice of the fixed-points

1444
01:26:01,984 --> 01:26:02,752
You can

1445
01:26:03,008 --> 01:26:05,568
Set the parameters and create those pics points

1446
01:26:05,824 --> 01:26:08,896
So that it will do content addressable memory will do

1447
01:26:09,152 --> 01:26:10,688
Cleaning up of a noisy

1448
01:26:10,944 --> 01:26:11,456
Memory

1449
01:26:12,224 --> 01:26:13,248
Moldova

1450
01:26:14,016 --> 01:26:20,160
You should have the following properties it should be possible if I give you an extra memory sio here's a fourth one I just thought

1451
01:26:20,416 --> 01:26:23,488
You should be able to make just a little tweak to the promises

1452
01:26:23,744 --> 01:26:24,768
So that the three

1453
01:26:25,280 --> 01:26:27,328
Memories that we were already that I still that

1454
01:26:27,584 --> 01:26:31,424
Just like I tell you something new and you don't forget everything that you for the love

1455
01:26:31,680 --> 01:26:36,288
The old memory should be preserved but you should learn the new Henry to you should have a new fixed point

1456
01:26:36,544 --> 01:26:40,384
So I just a little prick the parameters should magic me create a new fixed point

1457
01:26:41,920 --> 01:26:44,480
I want it to be able to drink alcohol and still work

1458
01:26:44,992 --> 01:26:47,296
So you'll dynamical system should be robust

1459
01:26:47,552 --> 01:26:50,112
2 corrupting more than half of the promises

1460
01:26:50,368 --> 01:26:52,416
But it should still working still have fixed points

1461
01:26:52,672 --> 01:26:53,696
How those three places

1462
01:26:54,208 --> 01:26:55,744
Okay that's the challenge

1463
01:26:56,000 --> 01:26:59,584
I9 curse you to try and solve this challenge before the next lecture

1464
01:26:59,840 --> 01:27:03,168
I didn't expect I'll show you a solution to this problem

1465
01:27:03,936 --> 01:27:04,704
Thanks for listening
